{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a5e6f4ce",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%pip install -r requirements.txt -q"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "bc922c9e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "import pandas as pd\n",
    "import json\n",
    "import xml.etree.ElementTree as ET\n",
    "\n",
    "# Namespace for ArXiv's Atom-based XML format.\n",
    "ARXIV_NAMESPACE = '{http://www.w3.org/2005/Atom}'\n",
    "\n",
    "def extract_from_arxiv(search_query='cat:cs.AI', max_results=100, json_file_path='files/arxiv_dataset.json'):\n",
    "    \"\"\"\n",
    "    Fetches papers from the ArXiv API based on a search query, saves them as JSON, \n",
    "    and returns a pandas DataFrame.\n",
    "\n",
    "    Args:\n",
    "        search_query (str): The search query for ArXiv (default is 'cat:cs.AI').\n",
    "        max_results (int): The maximum number of results to retrieve (default is 100).\n",
    "        json_file_path (str): File path where JSON data will be saved.\n",
    "\n",
    "    Returns:\n",
    "        pd.DataFrame: DataFrame containing the extracted paper information.\n",
    "    \"\"\"\n",
    "    \n",
    "    # Construct the URL for the API request.\n",
    "    url = f'http://export.arxiv.org/api/query?search_query={search_query}&max_results={max_results}'\n",
    "    \n",
    "    # Send a GET request to the ArXiv API.\n",
    "    response = requests.get(url)\n",
    "    response.raise_for_status()\n",
    "    \n",
    "    # Parse the XML response.\n",
    "    root = ET.fromstring(response.content)\n",
    "    \n",
    "    papers = []\n",
    "    \n",
    "    # Loop through each \"entry\" in the XML, representing a single paper.\n",
    "    for entry in root.findall(f'{ARXIV_NAMESPACE}entry'):\n",
    "        title_elem = entry.find(f'{ARXIV_NAMESPACE}title')\n",
    "        title = title_elem.text.strip() if title_elem is not None and title_elem.text else ''\n",
    "        \n",
    "        summary_elem = entry.find(f'{ARXIV_NAMESPACE}summary')\n",
    "        summary = summary_elem.text.strip() if summary_elem is not None and summary_elem.text else ''\n",
    "\n",
    "        # Get the authors of the paper.\n",
    "        author_elements = entry.findall(f'{ARXIV_NAMESPACE}author')\n",
    "        authors = []\n",
    "        for author in author_elements:\n",
    "            name_elem = author.find(f'{ARXIV_NAMESPACE}name')\n",
    "            if name_elem is not None and name_elem.text:\n",
    "                authors.append(name_elem.text)\n",
    "\n",
    "        # Get the paper's URL.\n",
    "        id_elem = entry.find(f'{ARXIV_NAMESPACE}id')\n",
    "        paper_url = id_elem.text if id_elem is not None and id_elem.text else ''\n",
    "        arxiv_id = paper_url.split('/')[-1] if paper_url else ''\n",
    "\n",
    "        # Check for the PDF link.\n",
    "        pdf_link = next((link.attrib['href'] for link in entry.findall(f'{ARXIV_NAMESPACE}link') \n",
    "                         if link.attrib.get('title') == 'pdf'), None)\n",
    "\n",
    "        papers.append({\n",
    "            'title': title,\n",
    "            'summary': summary,\n",
    "            'authors': authors,\n",
    "            'arxiv_id': arxiv_id,\n",
    "            'url': paper_url,\n",
    "            'pdf_link': pdf_link\n",
    "        })\n",
    "    \n",
    "    # Convert list into a pandas DataFrame.\n",
    "    df = pd.DataFrame(papers)\n",
    "    \n",
    "    # Save the DataFrame to a JSON file.\n",
    "    with open(json_file_path, 'w', encoding='utf-8') as f:\n",
    "        json.dump(papers, f, ensure_ascii=False, indent=4)\n",
    "        print(f'Data saved to {json_file_path} ...')\n",
    "    \n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e14fabd1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data saved to files/arxiv_dataset.json ...\n"
     ]
    }
   ],
   "source": [
    "df = extract_from_arxiv(max_results=20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "455400dd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[{'title': 'A Deep Reinforcement Learning Approach for Ramp Metering Based on Traffic Video Data', 'summary': 'Ramp metering that uses traffic signals to regulate vehicle flows from the on-ramps has been widely implemented to improve vehicle mobility of the freeway. Previous studies generally update signal timings in real-time based on predefined traffic measures collected by point detectors, such as traffic volumes and occupancies. Comparing with point detectors, traffic cameras-which have been increasingly deployed on road networks-could cover larger areas and provide more detailed traffic information. In this work, we propose a deep reinforcement learning (DRL) method to explore the potential of traffic video data in improving the efficiency of ramp metering. The proposed method uses traffic video frames as inputs and learns the optimal control strategies directly from the high-dimensional visual inputs. A real-world case study demonstrates that, in comparison with a state-of-the-practice method, the proposed DRL method results in 1) lower travel times in the mainline, 2) shorter vehicle queues at the on-ramp, and 3) higher traffic flows downstream of the merging area. The results suggest that the proposed method is able to extract useful information from the video data for better ramp metering controls.', 'authors': ['Bing Liu', 'Yu Tang', 'Yuxiong Ji', 'Yu Shen', 'Yuchuan Du'], 'arxiv_id': '2012.12104v1', 'url': 'http://arxiv.org/abs/2012.12104v1', 'pdf_link': 'https://arxiv.org/pdf/2012.12104v1'}, {'title': 'Rethink AI-based Power Grid Control: Diving Into Algorithm Design', 'summary': 'Recently, deep reinforcement learning (DRL)-based approach has shown promisein solving complex decision and control problems in power engineering domain.In this paper, we present an in-depth analysis of DRL-based voltage control fromaspects of algorithm selection, state space representation, and reward engineering.To resolve observed issues, we propose a novel imitation learning-based approachto directly map power grid operating points to effective actions without any interimreinforcement learning process. The performance results demonstrate that theproposed approach has strong generalization ability with much less training time.The agent trained by imitation learning is effective and robust to solve voltagecontrol problem and outperforms the former RL agents.', 'authors': ['Xiren Zhou', 'Siqi Wang', 'Ruisheng Diao', 'Desong Bian', 'Jiahui Duan', 'Di Shi'], 'arxiv_id': '2012.13026v1', 'url': 'http://arxiv.org/abs/2012.13026v1', 'pdf_link': 'https://arxiv.org/pdf/2012.13026v1'}, {'title': 'Fuzzy Commitments Offer Insufficient Protection to Biometric Templates Produced by Deep Learning', 'summary': 'In this work, we study the protection that fuzzy commitments offer when they are applied to facial images, processed by the state of the art deep learning facial recognition systems. We show that while these systems are capable of producing great accuracy, they produce templates of too little entropy. As a result, we present a reconstruction attack that takes a protected template, and reconstructs a facial image. The reconstructed facial images greatly resemble the original ones. In the simplest attack scenario, more than 78% of these reconstructed templates succeed in unlocking an account (when the system is configured to 0.1% FAR). Even in the \"hardest\" settings (in which we take a reconstructed image from one system and use it in a different system, with different feature extraction process) the reconstructed image offers 50 to 120 times higher success rates than the system\\'s FAR.', 'authors': ['Danny Keller', 'Margarita Osadchy', 'Orr Dunkelman'], 'arxiv_id': '2012.13293v1', 'url': 'http://arxiv.org/abs/2012.13293v1', 'pdf_link': 'https://arxiv.org/pdf/2012.13293v1'}, {'title': 'Generalization in portfolio-based algorithm selection', 'summary': \"Portfolio-based algorithm selection has seen tremendous practical success over the past two decades. This algorithm configuration procedure works by first selecting a portfolio of diverse algorithm parameter settings, and then, on a given problem instance, using an algorithm selector to choose a parameter setting from the portfolio with strong predicted performance. Oftentimes, both the portfolio and the algorithm selector are chosen using a training set of typical problem instances from the application domain at hand. In this paper, we provide the first provable guarantees for portfolio-based algorithm selection. We analyze how large the training set should be to ensure that the resulting algorithm selector's average performance over the training set is close to its future (expected) performance. This involves analyzing three key reasons why these two quantities may diverge: 1) the learning-theoretic complexity of the algorithm selector, 2) the size of the portfolio, and 3) the learning-theoretic complexity of the algorithm's performance as a function of its parameters. We introduce an end-to-end learning-theoretic analysis of the portfolio construction and algorithm selection together. We prove that if the portfolio is large, overfitting is inevitable, even with an extremely simple algorithm selector. With experiments, we illustrate a tradeoff exposed by our theoretical analysis: as we increase the portfolio size, we can hope to include a well-suited parameter setting for every possible problem instance, but it becomes impossible to avoid overfitting.\", 'authors': ['Maria-Florina Balcan', 'Tuomas Sandholm', 'Ellen Vitercik'], 'arxiv_id': '2012.13315v1', 'url': 'http://arxiv.org/abs/2012.13315v1', 'pdf_link': 'https://arxiv.org/pdf/2012.13315v1'}, {'title': 'I like fish, especially dolphins: Addressing Contradictions in Dialogue Modeling', 'summary': 'To quantify how well natural language understanding models can capture consistency in a general conversation, we introduce the DialoguE COntradiction DEtection task (DECODE) and a new conversational dataset containing both human-human and human-bot contradictory dialogues. We then compare a structured utterance-based approach of using pre-trained Transformer models for contradiction detection with the typical unstructured approach. Results reveal that: (i) our newly collected dataset is notably more effective at providing supervision for the dialogue contradiction detection task than existing NLI data including those aimed to cover the dialogue domain; (ii) the structured utterance-based approach is more robust and transferable on both analysis and out-of-distribution dialogues than its unstructured counterpart. We also show that our best contradiction detection model correlates well with human judgments and further provide evidence for its usage in both automatically evaluating and improving the consistency of state-of-the-art generative chatbots.', 'authors': ['Yixin Nie', 'Mary Williamson', 'Mohit Bansal', 'Douwe Kiela', 'Jason Weston'], 'arxiv_id': '2012.13391v2', 'url': 'http://arxiv.org/abs/2012.13391v2', 'pdf_link': 'https://arxiv.org/pdf/2012.13391v2'}, {'title': 'Skeleton-based Approaches based on Machine Vision: A Survey', 'summary': 'Recently, skeleton-based approaches have achieved rapid progress on the basis of great success in skeleton representation. Plenty of researches focus on solving specific problems according to skeleton features. Some skeleton-based approaches have been mentioned in several overviews on object detection as a non-essential part. Nevertheless, there has not been any thorough analysis of skeleton-based approaches attentively. Instead of describing these techniques in terms of theoretical constructs, we devote to summarizing skeleton-based approaches with regard to application fields and given tasks as comprehensively as possible. This paper is conducive to further understanding of skeleton-based application and dealing with particular issues.', 'authors': ['Jie Li', 'Binglin Li', 'Min Gao'], 'arxiv_id': '2012.12447v1', 'url': 'http://arxiv.org/abs/2012.12447v1', 'pdf_link': 'https://arxiv.org/pdf/2012.12447v1'}, {'title': 'Overview of FPGA deep learning acceleration based on convolutional neural network', 'summary': \"In recent years, deep learning has become more and more mature, and as a commonly used algorithm in deep learning, convolutional neural networks have been widely used in various visual tasks. In the past, research based on deep learning algorithms mainly relied on hardware such as GPUs and CPUs. However, with the increasing development of FPGAs, both field programmable logic gate arrays, it has become the main implementation hardware platform that combines various neural network deep learning algorithms This article is a review article, which mainly introduces the related theories and algorithms of convolution. It summarizes the application scenarios of several existing FPGA technologies based on convolutional neural networks, and mainly introduces the application of accelerators. At the same time, it summarizes some accelerators' under-utilization of logic resources or under-utilization of memory bandwidth, so that they can't get the best performance.\", 'authors': ['Simin Liu'], 'arxiv_id': '2012.12634v1', 'url': 'http://arxiv.org/abs/2012.12634v1', 'pdf_link': 'https://arxiv.org/pdf/2012.12634v1'}, {'title': 'Modelling Human Routines: Conceptualising Social Practice Theory for Agent-Based Simulation', 'summary': 'Our routines play an important role in a wide range of social challenges such as climate change, disease outbreaks and coordinating staff and patients in a hospital. To use agent-based simulations (ABS) to understand the role of routines in social challenges we need an agent framework that integrates routines. This paper provides the domain-independent Social Practice Agent (SoPrA) framework that satisfies requirements from the literature to simulate our routines. By choosing the appropriate concepts from the literature on agent theory, social psychology and social practice theory we ensure SoPrA correctly depicts current evidence on routines. By creating a consistent, modular and parsimonious framework suitable for multiple domains we enhance the usability of SoPrA. SoPrA provides ABS researchers with a conceptual, formal and computational framework to simulate routines and gain new insights into social systems.', 'authors': ['Rijk Mercuur', 'Virginia Dignum', 'Catholijn M. Jonker'], 'arxiv_id': '2012.11903v1', 'url': 'http://arxiv.org/abs/2012.11903v1', 'pdf_link': 'https://arxiv.org/pdf/2012.11903v1'}, {'title': 'Dynamic-K Recommendation with Personalized Decision Boundary', 'summary': \"In this paper, we investigate the recommendation task in the most common scenario with implicit feedback (e.g., clicks, purchases). State-of-the-art methods in this direction usually cast the problem as to learn a personalized ranking on a set of items (e.g., webpages, products). The top-N results are then provided to users as recommendations, where the N is usually a fixed number pre-defined by the system according to some heuristic criteria (e.g., page size, screen size). There is one major assumption underlying this fixed-number recommendation scheme, i.e., there are always sufficient relevant items to users' preferences. Unfortunately, this assumption may not always hold in real-world scenarios. In some applications, there might be very limited candidate items to recommend, and some users may have very high relevance requirement in recommendation. In this way, even the top-1 ranked item may not be relevant to a user's preference. Therefore, we argue that it is critical to provide a dynamic-K recommendation, where the K should be different with respect to the candidate item set and the target user. We formulate this dynamic-K recommendation task as a joint learning problem with both ranking and classification objectives. The ranking objective is the same as existing methods, i.e., to create a ranking list of items according to users' interests. The classification objective is unique in this work, which aims to learn a personalized decision boundary to differentiate the relevant items from irrelevant items. Based on these ideas, we extend two state-of-the-art ranking-based recommendation methods, i.e., BPRMF and HRM, to the corresponding dynamic-K versions, namely DK-BPRMF and DK-HRM. Our experimental results on two datasets show that the dynamic-K models are more effective than the original fixed-N recommendation methods.\", 'authors': ['Yan Gao', 'Jiafeng Guo', 'Yanyan Lan', 'Huaming Liao'], 'arxiv_id': '2012.13569v1', 'url': 'http://arxiv.org/abs/2012.13569v1', 'pdf_link': 'https://arxiv.org/pdf/2012.13569v1'}, {'title': 'Compliance Generation for Privacy Documents under GDPR: A Roadmap for Implementing Automation and Machine Learning', 'summary': 'Most prominent research today addresses compliance with data protection laws through consumer-centric and public-regulatory approaches. We shift this perspective with the Privatech project to focus on corporations and law firms as agents of compliance. To comply with data protection laws, data processors must implement accountability measures to assess and document compliance in relation to both privacy documents and privacy practices. In this paper, we survey, on the one hand, current research on GDPR automation, and on the other hand, the operational challenges corporations face to comply with GDPR, and that may benefit from new forms of automation. We attempt to bridge the gap. We provide a roadmap for compliance assessment and generation by identifying compliance issues, breaking them down into tasks that can be addressed through machine learning and automation, and providing notes about related developments in the Privatech project.', 'authors': ['David Restrepo Amariles', 'Aurore Clément Troussel', 'Rajaa El Hamdani'], 'arxiv_id': '2012.12718v1', 'url': 'http://arxiv.org/abs/2012.12718v1', 'pdf_link': 'https://arxiv.org/pdf/2012.12718v1'}, {'title': 'PaXNet: Dental Caries Detection in Panoramic X-ray using Ensemble Transfer Learning and Capsule Classifier', 'summary': \"Dental caries is one of the most chronic diseases involving the majority of the population during their lifetime. Caries lesions are typically diagnosed by radiologists relying only on their visual inspection to detect via dental x-rays. In many cases, dental caries is hard to identify using x-rays and can be misinterpreted as shadows due to different reasons such as low image quality. Hence, developing a decision support system for caries detection has been a topic of interest in recent years. Here, we propose an automatic diagnosis system to detect dental caries in Panoramic images for the first time, to the best of authors' knowledge. The proposed model benefits from various pretrained deep learning models through transfer learning to extract relevant features from x-rays and uses a capsule network to draw prediction results. On a dataset of 470 Panoramic images used for features extraction, including 240 labeled images for classification, our model achieved an accuracy score of 86.05\\\\% on the test set. The obtained score demonstrates acceptable detection performance and an increase in caries detection speed, as long as the challenges of using Panoramic x-rays of real patients are taken into account. Among images with caries lesions in the test set, our model acquired recall scores of 69.44\\\\% and 90.52\\\\% for mild and severe ones, confirming the fact that severe caries spots are more straightforward to detect and efficient mild caries detection needs a more robust and larger dataset. Considering the novelty of current research study as using Panoramic images, this work is a step towards developing a fully automated efficient decision support system to assist domain experts.\", 'authors': ['Arman Haghanifar', 'Mahdiyar Molahasani Majdabadi', 'Seok-Bum Ko'], 'arxiv_id': '2012.13666v1', 'url': 'http://arxiv.org/abs/2012.13666v1', 'pdf_link': 'https://arxiv.org/pdf/2012.13666v1'}, {'title': 'Toward Compact Data from Big Data', 'summary': 'Bigdata is a dataset of which size is beyond the ability of handling a valuable raw material that can be refined and distilled into valuable specific insights. Compact data is a method that optimizes the big dataset that gives best assets without handling complex bigdata. The compact dataset contains the maximum knowledge patterns at fine grained level for effective and personalized utilization of bigdata systems without bigdata. The compact data method is a tailor-made design which depends on problem situations. Various compact data techniques have been demonstrated into various data-driven research area in the paper.', 'authors': [' Song-Kyoo', ' Kim'], 'arxiv_id': '2012.13677v1', 'url': 'http://arxiv.org/abs/2012.13677v1', 'pdf_link': 'https://arxiv.org/pdf/2012.13677v1'}, {'title': 'Towards sample-efficient episodic control with DAC-ML', 'summary': 'The sample-inefficiency problem in Artificial Intelligence refers to the inability of current Deep Reinforcement Learning models to optimize action policies within a small number of episodes. Recent studies have tried to overcome this limitation by adding memory systems and architectural biases to improve learning speed, such as in Episodic Reinforcement Learning. However, despite achieving incremental improvements, their performance is still not comparable to how humans learn behavioral policies. In this paper, we capitalize on the design principles of the Distributed Adaptive Control (DAC) theory of mind and brain to build a novel cognitive architecture (DAC-ML) that, by incorporating a hippocampus-inspired sequential memory system, can rapidly converge to effective action policies that maximize reward acquisition in a challenging foraging task.', 'authors': ['Ismael T. Freire', 'Adrián F. Amil', 'Vasiliki Vouloutsi', 'Paul F. M. J. Verschure'], 'arxiv_id': '2012.13779v1', 'url': 'http://arxiv.org/abs/2012.13779v1', 'pdf_link': 'https://arxiv.org/pdf/2012.13779v1'}, {'title': 'My Teacher Thinks The World Is Flat! Interpreting Automatic Essay Scoring Mechanism', 'summary': \"Significant progress has been made in deep-learning based Automatic Essay Scoring (AES) systems in the past two decades. However, little research has been put to understand and interpret the black-box nature of these deep-learning based scoring models. Recent work shows that automated scoring systems are prone to even common-sense adversarial samples. Their lack of natural language understanding capability raises questions on the models being actively used by millions of candidates for life-changing decisions. With scoring being a highly multi-modal task, it becomes imperative for scoring models to be validated and tested on all these modalities. We utilize recent advances in interpretability to find the extent to which features such as coherence, content and relevance are important for automated scoring mechanisms and why they are susceptible to adversarial samples. We find that the systems tested consider essays not as a piece of prose having the characteristics of natural flow of speech and grammatical structure, but as `word-soups' where a few words are much more important than the other words. Removing the context surrounding those few important words causes the prose to lose the flow of speech and grammar, however has little impact on the predicted score. We also find that since the models are not semantically grounded with world-knowledge and common sense, adding false facts such as ``the world is flat'' actually increases the score instead of decreasing it.\", 'authors': ['Swapnil Parekh', 'Yaman Kumar Singla', 'Changyou Chen', 'Junyi Jessy Li', 'Rajiv Ratn Shah'], 'arxiv_id': '2012.13872v1', 'url': 'http://arxiv.org/abs/2012.13872v1', 'pdf_link': 'https://arxiv.org/pdf/2012.13872v1'}, {'title': 'Neural document expansion for ad-hoc information retrieval', 'summary': 'Recently, Nogueira et al. [2019] proposed a new approach to document expansion based on a neural Seq2Seq model, showing significant improvement on short text retrieval task. However, this approach needs a large amount of in-domain training data. In this paper, we show that this neural document expansion approach can be effectively adapted to standard IR tasks, where labels are scarce and many long documents are present.', 'authors': ['Cheng Tang', 'Andrew Arnold'], 'arxiv_id': '2012.14005v1', 'url': 'http://arxiv.org/abs/2012.14005v1', 'pdf_link': 'https://arxiv.org/pdf/2012.14005v1'}, {'title': 'Devil is in the Edges: Learning Semantic Boundaries from Noisy Annotations', 'summary': 'We tackle the problem of semantic boundary prediction, which aims to identify pixels that belong to object(class) boundaries. We notice that relevant datasets consist of a significant level of label noise, reflecting the fact that precise annotations are laborious to get and thus annotators trade-off quality with efficiency. We aim to learn sharp and precise semantic boundaries by explicitly reasoning about annotation noise during training. We propose a simple new layer and loss that can be used with existing learning-based boundary detectors. Our layer/loss enforces the detector to predict a maximum response along the normal direction at an edge, while also regularizing its direction. We further reason about true object boundaries during training using a level set formulation, which allows the network to learn from misaligned labels in an end-to-end fashion. Experiments show that we improve over the CASENet backbone network by more than 4% in terms of MF(ODS) and 18.61% in terms of AP, outperforming all current state-of-the-art methods including those that deal with alignment. Furthermore, we show that our learned network can be used to significantly improve coarse segmentation labels, lending itself as an efficient way to label new data.', 'authors': ['David Acuna', 'Amlan Kar', 'Sanja Fidler'], 'arxiv_id': '1904.07934v2', 'url': 'http://arxiv.org/abs/1904.07934v2', 'pdf_link': 'https://arxiv.org/pdf/1904.07934v2'}, {'title': 'How to define co-occurrence in different domains of study?', 'summary': 'This position paper presents a comparative study of co-occurrences. Some similarities and differences in the definition exist depending on the research domain (e.g. linguistics, NLP, computer science). This paper discusses these points, and deals with the methodological aspects in order to identify co-occurrences in a multidisciplinary paradigm.', 'authors': ['Mathieu Roche'], 'arxiv_id': '1904.08010v1', 'url': 'http://arxiv.org/abs/1904.08010v1', 'pdf_link': 'https://arxiv.org/pdf/1904.08010v1'}, {'title': 'Decision Making with Machine Learning and ROC Curves', 'summary': 'The Receiver Operating Characteristic (ROC) curve is a representation of the statistical information discovered in binary classification problems and is a key concept in machine learning and data science. This paper studies the statistical properties of ROC curves and its implication on model selection. We analyze the implications of different models of incentive heterogeneity and information asymmetry on the relation between human decisions and the ROC curves. Our theoretical discussion is illustrated in the context of a large data set of pregnancy outcomes and doctor diagnosis from the Pre-Pregnancy Checkups of reproductive age couples in Henan Province provided by the Chinese Ministry of Health.', 'authors': ['Kai Feng', 'Han Hong', 'Ke Tang', 'Jingyuan Wang'], 'arxiv_id': '1905.02810v1', 'url': 'http://arxiv.org/abs/1905.02810v1', 'pdf_link': 'https://arxiv.org/pdf/1905.02810v1'}, {'title': 'Feature Selection and Feature Extraction in Pattern Analysis: A Literature Review', 'summary': 'Pattern analysis often requires a pre-processing stage for extracting or selecting features in order to help the classification, prediction, or clustering stage discriminate or represent the data in a better way. The reason for this requirement is that the raw data are complex and difficult to process without extracting or selecting appropriate features beforehand. This paper reviews theory and motivation of different common methods of feature selection and extraction and introduces some of their applications. Some numerical implementations are also shown for these methods. Finally, the methods in feature selection and extraction are compared.', 'authors': ['Benyamin Ghojogh', 'Maria N. Samad', 'Sayema Asif Mashhadi', 'Tania Kapoor', 'Wahab Ali', 'Fakhri Karray', 'Mark Crowley'], 'arxiv_id': '1905.02845v1', 'url': 'http://arxiv.org/abs/1905.02845v1', 'pdf_link': 'https://arxiv.org/pdf/1905.02845v1'}, {'title': 'AI-Powered Text Generation for Harmonious Human-Machine Interaction: Current State and Future Directions', 'summary': 'In the last two decades, the landscape of text generation has undergone tremendous changes and is being reshaped by the success of deep learning. New technologies for text generation ranging from template-based methods to neural network-based methods emerged. Meanwhile, the research objectives have also changed from generating smooth and coherent sentences to infusing personalized traits to enrich the diversification of newly generated content. With the rapid development of text generation solutions, one comprehensive survey is urgent to summarize the achievements and track the state of the arts. In this survey paper, we present the general systematical framework, illustrate the widely utilized models and summarize the classic applications of text generation.', 'authors': ['Qiuyun Zhang', 'Bin Guo', 'Hao Wang', 'Yunji Liang', 'Shaoyang Hao', 'Zhiwen Yu'], 'arxiv_id': '1905.01984v1', 'url': 'http://arxiv.org/abs/1905.01984v1', 'pdf_link': 'https://arxiv.org/pdf/1905.01984v1'}]\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "file_name = 'files/arxiv_dataset.json'\n",
    "with  open(file_name, 'r') as file:\n",
    "    data = json.load(file)\n",
    "\n",
    "print(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "b613d15f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>title</th>\n",
       "      <th>summary</th>\n",
       "      <th>authors</th>\n",
       "      <th>arxiv_id</th>\n",
       "      <th>url</th>\n",
       "      <th>pdf_link</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>Neural document expansion for ad-hoc informati...</td>\n",
       "      <td>Recently, Nogueira et al. [2019] proposed a ne...</td>\n",
       "      <td>[Cheng Tang, Andrew Arnold]</td>\n",
       "      <td>2012.14005v1</td>\n",
       "      <td>http://arxiv.org/abs/2012.14005v1</td>\n",
       "      <td>https://arxiv.org/pdf/2012.14005v1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>AI-Powered Text Generation for Harmonious Huma...</td>\n",
       "      <td>In the last two decades, the landscape of text...</td>\n",
       "      <td>[Qiuyun Zhang, Bin Guo, Hao Wang, Yunji Liang,...</td>\n",
       "      <td>1905.01984v1</td>\n",
       "      <td>http://arxiv.org/abs/1905.01984v1</td>\n",
       "      <td>https://arxiv.org/pdf/1905.01984v1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>Toward Compact Data from Big Data</td>\n",
       "      <td>Bigdata is a dataset of which size is beyond t...</td>\n",
       "      <td>[ Song-Kyoo,&nbsp;&nbsp;Kim]</td>\n",
       "      <td>2012.13677v1</td>\n",
       "      <td>http://arxiv.org/abs/2012.13677v1</td>\n",
       "      <td>https://arxiv.org/pdf/2012.13677v1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>How to define co-occurrence in different domai...</td>\n",
       "      <td>This position paper presents a comparative stu...</td>\n",
       "      <td>[Mathieu Roche]</td>\n",
       "      <td>1904.08010v1</td>\n",
       "      <td>http://arxiv.org/abs/1904.08010v1</td>\n",
       "      <td>https://arxiv.org/pdf/1904.08010v1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>Dynamic-K Recommendation with Personalized Dec...</td>\n",
       "      <td>In this paper, we investigate the recommendati...</td>\n",
       "      <td>[Yan Gao, Jiafeng Guo, Yanyan Lan, Huaming Liao]</td>\n",
       "      <td>2012.13569v1</td>\n",
       "      <td>http://arxiv.org/abs/2012.13569v1</td>\n",
       "      <td>https://arxiv.org/pdf/2012.13569v1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                title  \\\n",
       "14  Neural document expansion for ad-hoc informati...   \n",
       "19  AI-Powered Text Generation for Harmonious Huma...   \n",
       "11                  Toward Compact Data from Big Data   \n",
       "16  How to define co-occurrence in different domai...   \n",
       "8   Dynamic-K Recommendation with Personalized Dec...   \n",
       "\n",
       "                                              summary  \\\n",
       "14  Recently, Nogueira et al. [2019] proposed a ne...   \n",
       "19  In the last two decades, the landscape of text...   \n",
       "11  Bigdata is a dataset of which size is beyond t...   \n",
       "16  This position paper presents a comparative stu...   \n",
       "8   In this paper, we investigate the recommendati...   \n",
       "\n",
       "                                              authors      arxiv_id  \\\n",
       "14                        [Cheng Tang, Andrew Arnold]  2012.14005v1   \n",
       "19  [Qiuyun Zhang, Bin Guo, Hao Wang, Yunji Liang,...  1905.01984v1   \n",
       "11                                 [ Song-Kyoo,  Kim]  2012.13677v1   \n",
       "16                                    [Mathieu Roche]  1904.08010v1   \n",
       "8    [Yan Gao, Jiafeng Guo, Yanyan Lan, Huaming Liao]  2012.13569v1   \n",
       "\n",
       "                                  url                            pdf_link  \n",
       "14  http://arxiv.org/abs/2012.14005v1  https://arxiv.org/pdf/2012.14005v1  \n",
       "19  http://arxiv.org/abs/1905.01984v1  https://arxiv.org/pdf/1905.01984v1  \n",
       "11  http://arxiv.org/abs/2012.13677v1  https://arxiv.org/pdf/2012.13677v1  \n",
       "16  http://arxiv.org/abs/1904.08010v1  https://arxiv.org/pdf/1904.08010v1  \n",
       "8   http://arxiv.org/abs/2012.13569v1  https://arxiv.org/pdf/2012.13569v1  "
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "df = pd.DataFrame(data)\n",
    "df.sample(n=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "f040d5f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import requests\n",
    "import os\n",
    "\n",
    "def download_pdfs(df, download_folder='files'):\n",
    "    \"\"\"\n",
    "    Downloads PDFs from URLs listed in the DataFrame and saves them to a specified folder. \n",
    "    The file names are stored in a new column 'pdf_file_name' in the DataFrame.\n",
    "    \n",
    "    Args:\n",
    "        df (pd.DataFrame): DataFrame containing a 'pdf_link' column with URLs to download.\n",
    "        download_folder (str): Path to the folder where PDFs will be saved (default is 'files').\n",
    "    \n",
    "    Returns:\n",
    "        pd.DataFrame: The original DataFrame with an additional 'pdf_file_name' column containing \n",
    "                      the paths of the downloaded PDF files or None if the download failed.\n",
    "    \"\"\"\n",
    "    \n",
    "    if not os.path.exists(download_folder):\n",
    "        os.makedirs(download_folder)\n",
    "    \n",
    "    pdf_file_names = []\n",
    "    \n",
    "    # Loop through each row to download PDFs\n",
    "    for index, row in df.iterrows():\n",
    "        pdf_link = row['pdf_link']\n",
    "        \n",
    "        try:\n",
    "            response = requests.get(pdf_link)\n",
    "            response.raise_for_status()\n",
    "    \n",
    "            file_name = os.path.join(download_folder, pdf_link.split('/')[-1]) + '.pdf'\n",
    "            pdf_file_names.append(file_name)\n",
    "    \n",
    "            # Save the downloaded PDF\n",
    "            with open(file_name, 'wb') as f:\n",
    "                f.write(response.content)\n",
    "            \n",
    "            print(f'PDF downloaded successfully and saved as {file_name}')\n",
    "        \n",
    "        except requests.exceptions.RequestException as e:\n",
    "            print(f'Failed to download the PDF: {e}')\n",
    "            pdf_file_names.append(None)\n",
    "    \n",
    "    df['pdf_file_name'] = pdf_file_names\n",
    "\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "fd9ef83a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PDF downloaded successfully and saved as files/2012.12104v1.pdf\n",
      "PDF downloaded successfully and saved as files/2012.13026v1.pdf\n",
      "PDF downloaded successfully and saved as files/2012.13293v1.pdf\n",
      "PDF downloaded successfully and saved as files/2012.13315v1.pdf\n",
      "PDF downloaded successfully and saved as files/2012.13391v2.pdf\n",
      "PDF downloaded successfully and saved as files/2012.12447v1.pdf\n",
      "PDF downloaded successfully and saved as files/2012.12634v1.pdf\n",
      "PDF downloaded successfully and saved as files/2012.11903v1.pdf\n",
      "PDF downloaded successfully and saved as files/2012.13569v1.pdf\n",
      "PDF downloaded successfully and saved as files/2012.12718v1.pdf\n",
      "PDF downloaded successfully and saved as files/2012.13666v1.pdf\n",
      "PDF downloaded successfully and saved as files/2012.13677v1.pdf\n",
      "PDF downloaded successfully and saved as files/2012.13779v1.pdf\n",
      "PDF downloaded successfully and saved as files/2012.13872v1.pdf\n",
      "PDF downloaded successfully and saved as files/2012.14005v1.pdf\n",
      "PDF downloaded successfully and saved as files/1904.07934v2.pdf\n",
      "PDF downloaded successfully and saved as files/1904.08010v1.pdf\n",
      "PDF downloaded successfully and saved as files/1905.02810v1.pdf\n",
      "PDF downloaded successfully and saved as files/1905.02845v1.pdf\n",
      "PDF downloaded successfully and saved as files/1905.01984v1.pdf\n"
     ]
    }
   ],
   "source": [
    "df = download_pdfs(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "4d03eb41",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>title</th>\n",
       "      <th>summary</th>\n",
       "      <th>authors</th>\n",
       "      <th>arxiv_id</th>\n",
       "      <th>url</th>\n",
       "      <th>pdf_link</th>\n",
       "      <th>pdf_file_name</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>A Deep Reinforcement Learning Approach for Ram...</td>\n",
       "      <td>Ramp metering that uses traffic signals to reg...</td>\n",
       "      <td>[Bing Liu, Yu Tang, Yuxiong Ji, Yu Shen, Yuchu...</td>\n",
       "      <td>2012.12104v1</td>\n",
       "      <td>http://arxiv.org/abs/2012.12104v1</td>\n",
       "      <td>https://arxiv.org/pdf/2012.12104v1</td>\n",
       "      <td>files/2012.12104v1.pdf</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Rethink AI-based Power Grid Control: Diving In...</td>\n",
       "      <td>Recently, deep reinforcement learning (DRL)-ba...</td>\n",
       "      <td>[Xiren Zhou, Siqi Wang, Ruisheng Diao, Desong ...</td>\n",
       "      <td>2012.13026v1</td>\n",
       "      <td>http://arxiv.org/abs/2012.13026v1</td>\n",
       "      <td>https://arxiv.org/pdf/2012.13026v1</td>\n",
       "      <td>files/2012.13026v1.pdf</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Fuzzy Commitments Offer Insufficient Protectio...</td>\n",
       "      <td>In this work, we study the protection that fuz...</td>\n",
       "      <td>[Danny Keller, Margarita Osadchy, Orr Dunkelman]</td>\n",
       "      <td>2012.13293v1</td>\n",
       "      <td>http://arxiv.org/abs/2012.13293v1</td>\n",
       "      <td>https://arxiv.org/pdf/2012.13293v1</td>\n",
       "      <td>files/2012.13293v1.pdf</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Generalization in portfolio-based algorithm se...</td>\n",
       "      <td>Portfolio-based algorithm selection has seen t...</td>\n",
       "      <td>[Maria-Florina Balcan, Tuomas Sandholm, Ellen ...</td>\n",
       "      <td>2012.13315v1</td>\n",
       "      <td>http://arxiv.org/abs/2012.13315v1</td>\n",
       "      <td>https://arxiv.org/pdf/2012.13315v1</td>\n",
       "      <td>files/2012.13315v1.pdf</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>I like fish, especially dolphins: Addressing C...</td>\n",
       "      <td>To quantify how well natural language understa...</td>\n",
       "      <td>[Yixin Nie, Mary Williamson, Mohit Bansal, Dou...</td>\n",
       "      <td>2012.13391v2</td>\n",
       "      <td>http://arxiv.org/abs/2012.13391v2</td>\n",
       "      <td>https://arxiv.org/pdf/2012.13391v2</td>\n",
       "      <td>files/2012.13391v2.pdf</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>Skeleton-based Approaches based on Machine Vis...</td>\n",
       "      <td>Recently, skeleton-based approaches have achie...</td>\n",
       "      <td>[Jie Li, Binglin Li, Min Gao]</td>\n",
       "      <td>2012.12447v1</td>\n",
       "      <td>http://arxiv.org/abs/2012.12447v1</td>\n",
       "      <td>https://arxiv.org/pdf/2012.12447v1</td>\n",
       "      <td>files/2012.12447v1.pdf</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>Overview of FPGA deep learning acceleration ba...</td>\n",
       "      <td>In recent years, deep learning has become more...</td>\n",
       "      <td>[Simin Liu]</td>\n",
       "      <td>2012.12634v1</td>\n",
       "      <td>http://arxiv.org/abs/2012.12634v1</td>\n",
       "      <td>https://arxiv.org/pdf/2012.12634v1</td>\n",
       "      <td>files/2012.12634v1.pdf</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>Modelling Human Routines: Conceptualising Soci...</td>\n",
       "      <td>Our routines play an important role in a wide ...</td>\n",
       "      <td>[Rijk Mercuur, Virginia Dignum, Catholijn M. J...</td>\n",
       "      <td>2012.11903v1</td>\n",
       "      <td>http://arxiv.org/abs/2012.11903v1</td>\n",
       "      <td>https://arxiv.org/pdf/2012.11903v1</td>\n",
       "      <td>files/2012.11903v1.pdf</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>Dynamic-K Recommendation with Personalized Dec...</td>\n",
       "      <td>In this paper, we investigate the recommendati...</td>\n",
       "      <td>[Yan Gao, Jiafeng Guo, Yanyan Lan, Huaming Liao]</td>\n",
       "      <td>2012.13569v1</td>\n",
       "      <td>http://arxiv.org/abs/2012.13569v1</td>\n",
       "      <td>https://arxiv.org/pdf/2012.13569v1</td>\n",
       "      <td>files/2012.13569v1.pdf</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>Compliance Generation for Privacy Documents un...</td>\n",
       "      <td>Most prominent research today addresses compli...</td>\n",
       "      <td>[David Restrepo Amariles, Aurore Clément Trous...</td>\n",
       "      <td>2012.12718v1</td>\n",
       "      <td>http://arxiv.org/abs/2012.12718v1</td>\n",
       "      <td>https://arxiv.org/pdf/2012.12718v1</td>\n",
       "      <td>files/2012.12718v1.pdf</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>PaXNet: Dental Caries Detection in Panoramic X...</td>\n",
       "      <td>Dental caries is one of the most chronic disea...</td>\n",
       "      <td>[Arman Haghanifar, Mahdiyar Molahasani Majdaba...</td>\n",
       "      <td>2012.13666v1</td>\n",
       "      <td>http://arxiv.org/abs/2012.13666v1</td>\n",
       "      <td>https://arxiv.org/pdf/2012.13666v1</td>\n",
       "      <td>files/2012.13666v1.pdf</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>Toward Compact Data from Big Data</td>\n",
       "      <td>Bigdata is a dataset of which size is beyond t...</td>\n",
       "      <td>[ Song-Kyoo,&nbsp;&nbsp;Kim]</td>\n",
       "      <td>2012.13677v1</td>\n",
       "      <td>http://arxiv.org/abs/2012.13677v1</td>\n",
       "      <td>https://arxiv.org/pdf/2012.13677v1</td>\n",
       "      <td>files/2012.13677v1.pdf</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>Towards sample-efficient episodic control with...</td>\n",
       "      <td>The sample-inefficiency problem in Artificial ...</td>\n",
       "      <td>[Ismael T. Freire, Adrián F. Amil, Vasiliki Vo...</td>\n",
       "      <td>2012.13779v1</td>\n",
       "      <td>http://arxiv.org/abs/2012.13779v1</td>\n",
       "      <td>https://arxiv.org/pdf/2012.13779v1</td>\n",
       "      <td>files/2012.13779v1.pdf</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>My Teacher Thinks The World Is Flat! Interpret...</td>\n",
       "      <td>Significant progress has been made in deep-lea...</td>\n",
       "      <td>[Swapnil Parekh, Yaman Kumar Singla, Changyou ...</td>\n",
       "      <td>2012.13872v1</td>\n",
       "      <td>http://arxiv.org/abs/2012.13872v1</td>\n",
       "      <td>https://arxiv.org/pdf/2012.13872v1</td>\n",
       "      <td>files/2012.13872v1.pdf</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>Neural document expansion for ad-hoc informati...</td>\n",
       "      <td>Recently, Nogueira et al. [2019] proposed a ne...</td>\n",
       "      <td>[Cheng Tang, Andrew Arnold]</td>\n",
       "      <td>2012.14005v1</td>\n",
       "      <td>http://arxiv.org/abs/2012.14005v1</td>\n",
       "      <td>https://arxiv.org/pdf/2012.14005v1</td>\n",
       "      <td>files/2012.14005v1.pdf</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>Devil is in the Edges: Learning Semantic Bound...</td>\n",
       "      <td>We tackle the problem of semantic boundary pre...</td>\n",
       "      <td>[David Acuna, Amlan Kar, Sanja Fidler]</td>\n",
       "      <td>1904.07934v2</td>\n",
       "      <td>http://arxiv.org/abs/1904.07934v2</td>\n",
       "      <td>https://arxiv.org/pdf/1904.07934v2</td>\n",
       "      <td>files/1904.07934v2.pdf</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>How to define co-occurrence in different domai...</td>\n",
       "      <td>This position paper presents a comparative stu...</td>\n",
       "      <td>[Mathieu Roche]</td>\n",
       "      <td>1904.08010v1</td>\n",
       "      <td>http://arxiv.org/abs/1904.08010v1</td>\n",
       "      <td>https://arxiv.org/pdf/1904.08010v1</td>\n",
       "      <td>files/1904.08010v1.pdf</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>Decision Making with Machine Learning and ROC ...</td>\n",
       "      <td>The Receiver Operating Characteristic (ROC) cu...</td>\n",
       "      <td>[Kai Feng, Han Hong, Ke Tang, Jingyuan Wang]</td>\n",
       "      <td>1905.02810v1</td>\n",
       "      <td>http://arxiv.org/abs/1905.02810v1</td>\n",
       "      <td>https://arxiv.org/pdf/1905.02810v1</td>\n",
       "      <td>files/1905.02810v1.pdf</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>Feature Selection and Feature Extraction in Pa...</td>\n",
       "      <td>Pattern analysis often requires a pre-processi...</td>\n",
       "      <td>[Benyamin Ghojogh, Maria N. Samad, Sayema Asif...</td>\n",
       "      <td>1905.02845v1</td>\n",
       "      <td>http://arxiv.org/abs/1905.02845v1</td>\n",
       "      <td>https://arxiv.org/pdf/1905.02845v1</td>\n",
       "      <td>files/1905.02845v1.pdf</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>AI-Powered Text Generation for Harmonious Huma...</td>\n",
       "      <td>In the last two decades, the landscape of text...</td>\n",
       "      <td>[Qiuyun Zhang, Bin Guo, Hao Wang, Yunji Liang,...</td>\n",
       "      <td>1905.01984v1</td>\n",
       "      <td>http://arxiv.org/abs/1905.01984v1</td>\n",
       "      <td>https://arxiv.org/pdf/1905.01984v1</td>\n",
       "      <td>files/1905.01984v1.pdf</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                title  \\\n",
       "0   A Deep Reinforcement Learning Approach for Ram...   \n",
       "1   Rethink AI-based Power Grid Control: Diving In...   \n",
       "2   Fuzzy Commitments Offer Insufficient Protectio...   \n",
       "3   Generalization in portfolio-based algorithm se...   \n",
       "4   I like fish, especially dolphins: Addressing C...   \n",
       "5   Skeleton-based Approaches based on Machine Vis...   \n",
       "6   Overview of FPGA deep learning acceleration ba...   \n",
       "7   Modelling Human Routines: Conceptualising Soci...   \n",
       "8   Dynamic-K Recommendation with Personalized Dec...   \n",
       "9   Compliance Generation for Privacy Documents un...   \n",
       "10  PaXNet: Dental Caries Detection in Panoramic X...   \n",
       "11                  Toward Compact Data from Big Data   \n",
       "12  Towards sample-efficient episodic control with...   \n",
       "13  My Teacher Thinks The World Is Flat! Interpret...   \n",
       "14  Neural document expansion for ad-hoc informati...   \n",
       "15  Devil is in the Edges: Learning Semantic Bound...   \n",
       "16  How to define co-occurrence in different domai...   \n",
       "17  Decision Making with Machine Learning and ROC ...   \n",
       "18  Feature Selection and Feature Extraction in Pa...   \n",
       "19  AI-Powered Text Generation for Harmonious Huma...   \n",
       "\n",
       "                                              summary  \\\n",
       "0   Ramp metering that uses traffic signals to reg...   \n",
       "1   Recently, deep reinforcement learning (DRL)-ba...   \n",
       "2   In this work, we study the protection that fuz...   \n",
       "3   Portfolio-based algorithm selection has seen t...   \n",
       "4   To quantify how well natural language understa...   \n",
       "5   Recently, skeleton-based approaches have achie...   \n",
       "6   In recent years, deep learning has become more...   \n",
       "7   Our routines play an important role in a wide ...   \n",
       "8   In this paper, we investigate the recommendati...   \n",
       "9   Most prominent research today addresses compli...   \n",
       "10  Dental caries is one of the most chronic disea...   \n",
       "11  Bigdata is a dataset of which size is beyond t...   \n",
       "12  The sample-inefficiency problem in Artificial ...   \n",
       "13  Significant progress has been made in deep-lea...   \n",
       "14  Recently, Nogueira et al. [2019] proposed a ne...   \n",
       "15  We tackle the problem of semantic boundary pre...   \n",
       "16  This position paper presents a comparative stu...   \n",
       "17  The Receiver Operating Characteristic (ROC) cu...   \n",
       "18  Pattern analysis often requires a pre-processi...   \n",
       "19  In the last two decades, the landscape of text...   \n",
       "\n",
       "                                              authors      arxiv_id  \\\n",
       "0   [Bing Liu, Yu Tang, Yuxiong Ji, Yu Shen, Yuchu...  2012.12104v1   \n",
       "1   [Xiren Zhou, Siqi Wang, Ruisheng Diao, Desong ...  2012.13026v1   \n",
       "2    [Danny Keller, Margarita Osadchy, Orr Dunkelman]  2012.13293v1   \n",
       "3   [Maria-Florina Balcan, Tuomas Sandholm, Ellen ...  2012.13315v1   \n",
       "4   [Yixin Nie, Mary Williamson, Mohit Bansal, Dou...  2012.13391v2   \n",
       "5                       [Jie Li, Binglin Li, Min Gao]  2012.12447v1   \n",
       "6                                         [Simin Liu]  2012.12634v1   \n",
       "7   [Rijk Mercuur, Virginia Dignum, Catholijn M. J...  2012.11903v1   \n",
       "8    [Yan Gao, Jiafeng Guo, Yanyan Lan, Huaming Liao]  2012.13569v1   \n",
       "9   [David Restrepo Amariles, Aurore Clément Trous...  2012.12718v1   \n",
       "10  [Arman Haghanifar, Mahdiyar Molahasani Majdaba...  2012.13666v1   \n",
       "11                                 [ Song-Kyoo,  Kim]  2012.13677v1   \n",
       "12  [Ismael T. Freire, Adrián F. Amil, Vasiliki Vo...  2012.13779v1   \n",
       "13  [Swapnil Parekh, Yaman Kumar Singla, Changyou ...  2012.13872v1   \n",
       "14                        [Cheng Tang, Andrew Arnold]  2012.14005v1   \n",
       "15             [David Acuna, Amlan Kar, Sanja Fidler]  1904.07934v2   \n",
       "16                                    [Mathieu Roche]  1904.08010v1   \n",
       "17       [Kai Feng, Han Hong, Ke Tang, Jingyuan Wang]  1905.02810v1   \n",
       "18  [Benyamin Ghojogh, Maria N. Samad, Sayema Asif...  1905.02845v1   \n",
       "19  [Qiuyun Zhang, Bin Guo, Hao Wang, Yunji Liang,...  1905.01984v1   \n",
       "\n",
       "                                  url                            pdf_link  \\\n",
       "0   http://arxiv.org/abs/2012.12104v1  https://arxiv.org/pdf/2012.12104v1   \n",
       "1   http://arxiv.org/abs/2012.13026v1  https://arxiv.org/pdf/2012.13026v1   \n",
       "2   http://arxiv.org/abs/2012.13293v1  https://arxiv.org/pdf/2012.13293v1   \n",
       "3   http://arxiv.org/abs/2012.13315v1  https://arxiv.org/pdf/2012.13315v1   \n",
       "4   http://arxiv.org/abs/2012.13391v2  https://arxiv.org/pdf/2012.13391v2   \n",
       "5   http://arxiv.org/abs/2012.12447v1  https://arxiv.org/pdf/2012.12447v1   \n",
       "6   http://arxiv.org/abs/2012.12634v1  https://arxiv.org/pdf/2012.12634v1   \n",
       "7   http://arxiv.org/abs/2012.11903v1  https://arxiv.org/pdf/2012.11903v1   \n",
       "8   http://arxiv.org/abs/2012.13569v1  https://arxiv.org/pdf/2012.13569v1   \n",
       "9   http://arxiv.org/abs/2012.12718v1  https://arxiv.org/pdf/2012.12718v1   \n",
       "10  http://arxiv.org/abs/2012.13666v1  https://arxiv.org/pdf/2012.13666v1   \n",
       "11  http://arxiv.org/abs/2012.13677v1  https://arxiv.org/pdf/2012.13677v1   \n",
       "12  http://arxiv.org/abs/2012.13779v1  https://arxiv.org/pdf/2012.13779v1   \n",
       "13  http://arxiv.org/abs/2012.13872v1  https://arxiv.org/pdf/2012.13872v1   \n",
       "14  http://arxiv.org/abs/2012.14005v1  https://arxiv.org/pdf/2012.14005v1   \n",
       "15  http://arxiv.org/abs/1904.07934v2  https://arxiv.org/pdf/1904.07934v2   \n",
       "16  http://arxiv.org/abs/1904.08010v1  https://arxiv.org/pdf/1904.08010v1   \n",
       "17  http://arxiv.org/abs/1905.02810v1  https://arxiv.org/pdf/1905.02810v1   \n",
       "18  http://arxiv.org/abs/1905.02845v1  https://arxiv.org/pdf/1905.02845v1   \n",
       "19  http://arxiv.org/abs/1905.01984v1  https://arxiv.org/pdf/1905.01984v1   \n",
       "\n",
       "             pdf_file_name  \n",
       "0   files/2012.12104v1.pdf  \n",
       "1   files/2012.13026v1.pdf  \n",
       "2   files/2012.13293v1.pdf  \n",
       "3   files/2012.13315v1.pdf  \n",
       "4   files/2012.13391v2.pdf  \n",
       "5   files/2012.12447v1.pdf  \n",
       "6   files/2012.12634v1.pdf  \n",
       "7   files/2012.11903v1.pdf  \n",
       "8   files/2012.13569v1.pdf  \n",
       "9   files/2012.12718v1.pdf  \n",
       "10  files/2012.13666v1.pdf  \n",
       "11  files/2012.13677v1.pdf  \n",
       "12  files/2012.13779v1.pdf  \n",
       "13  files/2012.13872v1.pdf  \n",
       "14  files/2012.14005v1.pdf  \n",
       "15  files/1904.07934v2.pdf  \n",
       "16  files/1904.08010v1.pdf  \n",
       "17  files/1905.02810v1.pdf  \n",
       "18  files/1905.02845v1.pdf  \n",
       "19  files/1905.01984v1.pdf  "
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "6cf0de68",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_community.document_loaders import PyPDFLoader\n",
    "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
    "\n",
    "def load_and_chunk_pdf(pdf_file_name, chunk_size=512):\n",
    "    \"\"\"\n",
    "    Loads a PDF file and splits its content into chunks of a specified size.\n",
    "\n",
    "    Args:\n",
    "        file (str): Path to the PDF file to be loaded.\n",
    "        chunk_size (int): The maximum size of each chunk in characters (default is 512).\n",
    "\n",
    "    Returns:\n",
    "        List[Document]: A list of document chunks.\n",
    "    \"\"\"\n",
    "\n",
    "    print(f'Loading and splitting into chunks: {pdf_file_name}')\n",
    "\n",
    "    # Load the content of the PDF\n",
    "    loader = PyPDFLoader(pdf_file_name)\n",
    "    data = loader.load()\n",
    "\n",
    "    # Split the content into chunks with slight overlap to preserve context\n",
    "    text_splitter = RecursiveCharacterTextSplitter(chunk_size=chunk_size, chunk_overlap=64)\n",
    "    chunks = text_splitter.split_documents(data)\n",
    "\n",
    "    return chunks\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "da529fa9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def expand_df(df):\n",
    "    \"\"\"\n",
    "    Expands each row in the DataFrame by splitting PDF documents into chunks.\n",
    "\n",
    "    Args:\n",
    "        df (pd.DataFrame): DataFrame containing 'pdf_file_name', 'arxiv_id', 'title', 'summary', \n",
    "                           'authors', and 'url' columns.\n",
    "\n",
    "    Returns:\n",
    "        pd.DataFrame: A new DataFrame where each row represents a chunk of the original document, \n",
    "                      with additional metadata such as chunk identifiers and relationships to \n",
    "                      adjacent chunks.\n",
    "    \"\"\"\n",
    "\n",
    "    expanded_rows = []  # List to store expanded rows with chunk information\n",
    "\n",
    "    # Loop through each row in the DataFrame\n",
    "    for idx, row in df.iterrows():\n",
    "        try:\n",
    "            chunks = load_and_chunk_pdf(row['pdf_file_name'])\n",
    "        except Exception as e:\n",
    "            print(f\"Error processing file {row['pdf_file_name']}: {e}\")\n",
    "            continue\n",
    "\n",
    "        # Loop over the chunks and construct a new DataFrame row for each\n",
    "        for i, chunk in enumerate(chunks):\n",
    "            prechunk_id = i-1 if i > 0 else ''  # Preceding chunk ID\n",
    "            postchunk_id = i+1 if i < len(chunks) - 1 else ''  # Following chunk ID\n",
    "\n",
    "            expanded_rows.append({\n",
    "                'id': f\"{row['arxiv_id']}#{i}\",  # Unique chunk identifier\n",
    "                'title': row['title'],\n",
    "                'summary': row['summary'],\n",
    "                'authors': row['authors'],\n",
    "                'arxiv_id': row['arxiv_id'],\n",
    "                'url': row['url'],\n",
    "                'chunk': chunk.page_content,  # Text content of the chunk\n",
    "                'prechunk_id': '' if i == 0 else f\"{row['arxiv_id']}#{prechunk_id}\",  # Previous chunk ID\n",
    "                'postchunk_id': '' if i == len(chunks) - 1 else f\"{row['arxiv_id']}#{postchunk_id}\"  # Next chunk ID\n",
    "            })\n",
    "\n",
    "    # Return a new expanded DataFrame\n",
    "    return pd.DataFrame(expanded_rows)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "ac407d82",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading and splitting into chunks: files/2012.12104v1.pdf\n",
      "Loading and splitting into chunks: files/2012.13026v1.pdf\n",
      "Loading and splitting into chunks: files/2012.13293v1.pdf\n",
      "Loading and splitting into chunks: files/2012.13315v1.pdf\n",
      "Loading and splitting into chunks: files/2012.13391v2.pdf\n",
      "Loading and splitting into chunks: files/2012.12447v1.pdf\n",
      "Loading and splitting into chunks: files/2012.12634v1.pdf\n",
      "Loading and splitting into chunks: files/2012.11903v1.pdf\n",
      "Loading and splitting into chunks: files/2012.13569v1.pdf\n",
      "Loading and splitting into chunks: files/2012.12718v1.pdf\n",
      "Loading and splitting into chunks: files/2012.13666v1.pdf\n",
      "Loading and splitting into chunks: files/2012.13677v1.pdf\n",
      "Loading and splitting into chunks: files/2012.13779v1.pdf\n",
      "Loading and splitting into chunks: files/2012.13872v1.pdf\n",
      "Loading and splitting into chunks: files/2012.14005v1.pdf\n",
      "Loading and splitting into chunks: files/1904.07934v2.pdf\n",
      "Loading and splitting into chunks: files/1904.08010v1.pdf\n",
      "Loading and splitting into chunks: files/1905.02810v1.pdf\n",
      "Loading and splitting into chunks: files/1905.02845v1.pdf\n",
      "Loading and splitting into chunks: files/1905.01984v1.pdf\n"
     ]
    }
   ],
   "source": [
    "expanded_df = expand_df(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "fc9d8413",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>title</th>\n",
       "      <th>summary</th>\n",
       "      <th>authors</th>\n",
       "      <th>arxiv_id</th>\n",
       "      <th>url</th>\n",
       "      <th>chunk</th>\n",
       "      <th>prechunk_id</th>\n",
       "      <th>postchunk_id</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2012.12104v1#0</td>\n",
       "      <td>A Deep Reinforcement Learning Approach for Ram...</td>\n",
       "      <td>Ramp metering that uses traffic signals to reg...</td>\n",
       "      <td>[Bing Liu, Yu Tang, Yuxiong Ji, Yu Shen, Yuchu...</td>\n",
       "      <td>2012.12104v1</td>\n",
       "      <td>http://arxiv.org/abs/2012.12104v1</td>\n",
       "      <td>1 \\nA Deep Reinforcement Learning Approach for...</td>\n",
       "      <td></td>\n",
       "      <td>2012.12104v1#1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2012.12104v1#1</td>\n",
       "      <td>A Deep Reinforcement Learning Approach for Ram...</td>\n",
       "      <td>Ramp metering that uses traffic signals to reg...</td>\n",
       "      <td>[Bing Liu, Yu Tang, Yuxiong Ji, Yu Shen, Yuchu...</td>\n",
       "      <td>2012.12104v1</td>\n",
       "      <td>http://arxiv.org/abs/2012.12104v1</td>\n",
       "      <td>Abstract \\nRamp metering that uses traffic sig...</td>\n",
       "      <td>2012.12104v1#0</td>\n",
       "      <td>2012.12104v1#2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2012.12104v1#2</td>\n",
       "      <td>A Deep Reinforcement Learning Approach for Ram...</td>\n",
       "      <td>Ramp metering that uses traffic signals to reg...</td>\n",
       "      <td>[Bing Liu, Yu Tang, Yuxiong Ji, Yu Shen, Yuchu...</td>\n",
       "      <td>2012.12104v1</td>\n",
       "      <td>http://arxiv.org/abs/2012.12104v1</td>\n",
       "      <td>and provide more detailed traffic information....</td>\n",
       "      <td>2012.12104v1#1</td>\n",
       "      <td>2012.12104v1#3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2012.12104v1#3</td>\n",
       "      <td>A Deep Reinforcement Learning Approach for Ram...</td>\n",
       "      <td>Ramp metering that uses traffic signals to reg...</td>\n",
       "      <td>[Bing Liu, Yu Tang, Yuxiong Ji, Yu Shen, Yuchu...</td>\n",
       "      <td>2012.12104v1</td>\n",
       "      <td>http://arxiv.org/abs/2012.12104v1</td>\n",
       "      <td>method results in 1) lower travel times in the...</td>\n",
       "      <td>2012.12104v1#2</td>\n",
       "      <td>2012.12104v1#4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2012.12104v1#4</td>\n",
       "      <td>A Deep Reinforcement Learning Approach for Ram...</td>\n",
       "      <td>Ramp metering that uses traffic signals to reg...</td>\n",
       "      <td>[Bing Liu, Yu Tang, Yuxiong Ji, Yu Shen, Yuchu...</td>\n",
       "      <td>2012.12104v1</td>\n",
       "      <td>http://arxiv.org/abs/2012.12104v1</td>\n",
       "      <td>2 \\nIntroduction \\nRamp metering uses traffic ...</td>\n",
       "      <td>2012.12104v1#3</td>\n",
       "      <td>2012.12104v1#5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2195</th>\n",
       "      <td>1905.01984v1#123</td>\n",
       "      <td>AI-Powered Text Generation for Harmonious Huma...</td>\n",
       "      <td>In the last two decades, the landscape of text...</td>\n",
       "      <td>[Qiuyun Zhang, Bin Guo, Hao Wang, Yunji Liang,...</td>\n",
       "      <td>1905.01984v1</td>\n",
       "      <td>http://arxiv.org/abs/1905.01984v1</td>\n",
       "      <td>Topic aware neural response generation . In Th...</td>\n",
       "      <td>1905.01984v1#122</td>\n",
       "      <td>1905.01984v1#124</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2196</th>\n",
       "      <td>1905.01984v1#124</td>\n",
       "      <td>AI-Powered Text Generation for Harmonious Huma...</td>\n",
       "      <td>In the last two decades, the landscape of text...</td>\n",
       "      <td>[Qiuyun Zhang, Bin Guo, Hao Wang, Yunji Liang,...</td>\n",
       "      <td>1905.01984v1</td>\n",
       "      <td>http://arxiv.org/abs/1905.01984v1</td>\n",
       "      <td>Personalized response generation via domain ad...</td>\n",
       "      <td>1905.01984v1#123</td>\n",
       "      <td>1905.01984v1#125</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2197</th>\n",
       "      <td>1905.01984v1#125</td>\n",
       "      <td>AI-Powered Text Generation for Harmonious Huma...</td>\n",
       "      <td>In the last two decades, the landscape of text...</td>\n",
       "      <td>[Qiuyun Zhang, Bin Guo, Hao Wang, Yunji Liang,...</td>\n",
       "      <td>1905.01984v1</td>\n",
       "      <td>http://arxiv.org/abs/1905.01984v1</td>\n",
       "      <td>generative adversarial nets with policy gradie...</td>\n",
       "      <td>1905.01984v1#124</td>\n",
       "      <td>1905.01984v1#126</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2198</th>\n",
       "      <td>1905.01984v1#126</td>\n",
       "      <td>AI-Powered Text Generation for Harmonious Huma...</td>\n",
       "      <td>In the last two decades, the landscape of text...</td>\n",
       "      <td>[Qiuyun Zhang, Bin Guo, Hao Wang, Yunji Liang,...</td>\n",
       "      <td>1905.01984v1</td>\n",
       "      <td>http://arxiv.org/abs/1905.01984v1</td>\n",
       "      <td>pets too? arXiv preprint arXiv:1801.07243.&nbsp;&nbsp;\\n...</td>\n",
       "      <td>1905.01984v1#125</td>\n",
       "      <td>1905.01984v1#127</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2199</th>\n",
       "      <td>1905.01984v1#127</td>\n",
       "      <td>AI-Powered Text Generation for Harmonious Huma...</td>\n",
       "      <td>In the last two decades, the landscape of text...</td>\n",
       "      <td>[Qiuyun Zhang, Bin Guo, Hao Wang, Yunji Liang,...</td>\n",
       "      <td>1905.01984v1</td>\n",
       "      <td>http://arxiv.org/abs/1905.01984v1</td>\n",
       "      <td>(2018). Multi-turn response selection for chat...</td>\n",
       "      <td>1905.01984v1#126</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>2200 rows × 9 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                    id                                              title  \\\n",
       "0       2012.12104v1#0  A Deep Reinforcement Learning Approach for Ram...   \n",
       "1       2012.12104v1#1  A Deep Reinforcement Learning Approach for Ram...   \n",
       "2       2012.12104v1#2  A Deep Reinforcement Learning Approach for Ram...   \n",
       "3       2012.12104v1#3  A Deep Reinforcement Learning Approach for Ram...   \n",
       "4       2012.12104v1#4  A Deep Reinforcement Learning Approach for Ram...   \n",
       "...                ...                                                ...   \n",
       "2195  1905.01984v1#123  AI-Powered Text Generation for Harmonious Huma...   \n",
       "2196  1905.01984v1#124  AI-Powered Text Generation for Harmonious Huma...   \n",
       "2197  1905.01984v1#125  AI-Powered Text Generation for Harmonious Huma...   \n",
       "2198  1905.01984v1#126  AI-Powered Text Generation for Harmonious Huma...   \n",
       "2199  1905.01984v1#127  AI-Powered Text Generation for Harmonious Huma...   \n",
       "\n",
       "                                                summary  \\\n",
       "0     Ramp metering that uses traffic signals to reg...   \n",
       "1     Ramp metering that uses traffic signals to reg...   \n",
       "2     Ramp metering that uses traffic signals to reg...   \n",
       "3     Ramp metering that uses traffic signals to reg...   \n",
       "4     Ramp metering that uses traffic signals to reg...   \n",
       "...                                                 ...   \n",
       "2195  In the last two decades, the landscape of text...   \n",
       "2196  In the last two decades, the landscape of text...   \n",
       "2197  In the last two decades, the landscape of text...   \n",
       "2198  In the last two decades, the landscape of text...   \n",
       "2199  In the last two decades, the landscape of text...   \n",
       "\n",
       "                                                authors      arxiv_id  \\\n",
       "0     [Bing Liu, Yu Tang, Yuxiong Ji, Yu Shen, Yuchu...  2012.12104v1   \n",
       "1     [Bing Liu, Yu Tang, Yuxiong Ji, Yu Shen, Yuchu...  2012.12104v1   \n",
       "2     [Bing Liu, Yu Tang, Yuxiong Ji, Yu Shen, Yuchu...  2012.12104v1   \n",
       "3     [Bing Liu, Yu Tang, Yuxiong Ji, Yu Shen, Yuchu...  2012.12104v1   \n",
       "4     [Bing Liu, Yu Tang, Yuxiong Ji, Yu Shen, Yuchu...  2012.12104v1   \n",
       "...                                                 ...           ...   \n",
       "2195  [Qiuyun Zhang, Bin Guo, Hao Wang, Yunji Liang,...  1905.01984v1   \n",
       "2196  [Qiuyun Zhang, Bin Guo, Hao Wang, Yunji Liang,...  1905.01984v1   \n",
       "2197  [Qiuyun Zhang, Bin Guo, Hao Wang, Yunji Liang,...  1905.01984v1   \n",
       "2198  [Qiuyun Zhang, Bin Guo, Hao Wang, Yunji Liang,...  1905.01984v1   \n",
       "2199  [Qiuyun Zhang, Bin Guo, Hao Wang, Yunji Liang,...  1905.01984v1   \n",
       "\n",
       "                                    url  \\\n",
       "0     http://arxiv.org/abs/2012.12104v1   \n",
       "1     http://arxiv.org/abs/2012.12104v1   \n",
       "2     http://arxiv.org/abs/2012.12104v1   \n",
       "3     http://arxiv.org/abs/2012.12104v1   \n",
       "4     http://arxiv.org/abs/2012.12104v1   \n",
       "...                                 ...   \n",
       "2195  http://arxiv.org/abs/1905.01984v1   \n",
       "2196  http://arxiv.org/abs/1905.01984v1   \n",
       "2197  http://arxiv.org/abs/1905.01984v1   \n",
       "2198  http://arxiv.org/abs/1905.01984v1   \n",
       "2199  http://arxiv.org/abs/1905.01984v1   \n",
       "\n",
       "                                                  chunk       prechunk_id  \\\n",
       "0     1 \\nA Deep Reinforcement Learning Approach for...                     \n",
       "1     Abstract \\nRamp metering that uses traffic sig...    2012.12104v1#0   \n",
       "2     and provide more detailed traffic information....    2012.12104v1#1   \n",
       "3     method results in 1) lower travel times in the...    2012.12104v1#2   \n",
       "4     2 \\nIntroduction \\nRamp metering uses traffic ...    2012.12104v1#3   \n",
       "...                                                 ...               ...   \n",
       "2195  Topic aware neural response generation . In Th...  1905.01984v1#122   \n",
       "2196  Personalized response generation via domain ad...  1905.01984v1#123   \n",
       "2197  generative adversarial nets with policy gradie...  1905.01984v1#124   \n",
       "2198  pets too? arXiv preprint arXiv:1801.07243.  \\n...  1905.01984v1#125   \n",
       "2199  (2018). Multi-turn response selection for chat...  1905.01984v1#126   \n",
       "\n",
       "          postchunk_id  \n",
       "0       2012.12104v1#1  \n",
       "1       2012.12104v1#2  \n",
       "2       2012.12104v1#3  \n",
       "3       2012.12104v1#4  \n",
       "4       2012.12104v1#5  \n",
       "...                ...  \n",
       "2195  1905.01984v1#124  \n",
       "2196  1905.01984v1#125  \n",
       "2197  1905.01984v1#126  \n",
       "2198  1905.01984v1#127  \n",
       "2199                    \n",
       "\n",
       "[2200 rows x 9 columns]"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "expanded_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "5d2d0d1f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from dotenv import load_dotenv, find_dotenv\n",
    "\n",
    "# Load the API keys from .env\n",
    "load_dotenv(find_dotenv(), override=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "d929e8f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from getpass import getpass\n",
    "\n",
    "from semantic_router.encoders import OpenAIEncoder\n",
    "\n",
    "# Check if 'OPENAI_API_KEY' is set; prompt if not\n",
    "os.environ['OPENAI_API_KEY'] = os.getenv('OPENAI_API_KEY') or getpass('OpenAI API key: ')\n",
    "\n",
    "# Initialize the OpenAIEncoder with a specific model\n",
    "encoder = OpenAIEncoder(name='text-embedding-3-small')  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "18f992fa",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[0.018583279103040695,\n",
       "  -0.025987325236201286,\n",
       "  0.01025006826967001,\n",
       "  -0.031957387924194336,\n",
       "  -0.004148314706981182,\n",
       "  -0.04852138087153435,\n",
       "  -0.01638840325176716,\n",
       "  0.04334147274494171,\n",
       "  -0.03148914501070976,\n",
       "  -0.018451586365699768,\n",
       "  0.0052201454527676105,\n",
       "  -0.08088847994804382,\n",
       "  0.005318915005773306,\n",
       "  -0.012496157549321651,\n",
       "  -0.010967060923576355,\n",
       "  0.09593068808317184,\n",
       "  -0.0387176014482975,\n",
       "  0.025460554286837578,\n",
       "  -0.0030234409496188164,\n",
       "  0.04275617375969887,\n",
       "  0.04527296498417854,\n",
       "  0.011076805181801319,\n",
       "  -0.0395662896335125,\n",
       "  0.03997599706053734,\n",
       "  0.03169400244951248,\n",
       "  0.029118681326508522,\n",
       "  0.024977682158350945,\n",
       "  0.015583615750074387,\n",
       "  0.014076467603445053,\n",
       "  -0.013761868700385094,\n",
       "  0.04544855281710625,\n",
       "  -0.05346716567873955,\n",
       "  -0.01498368289321661,\n",
       "  0.025050844997167587,\n",
       "  -0.0031313556246459484,\n",
       "  -0.020851315930485725,\n",
       "  -0.024348484352231026,\n",
       "  0.009284323081374168,\n",
       "  0.007041892036795616,\n",
       "  -0.016915174201130867,\n",
       "  -0.0011724292999133468,\n",
       "  -0.03306945785880089,\n",
       "  0.07006042450666428,\n",
       "  0.03614228218793869,\n",
       "  0.006610233336687088,\n",
       "  -0.0475848987698555,\n",
       "  -0.02666042000055313,\n",
       "  0.014654451981186867,\n",
       "  0.007155294064432383,\n",
       "  0.0031002615578472614,\n",
       "  0.0023192516528069973,\n",
       "  0.025167904794216156,\n",
       "  0.029191844165325165,\n",
       "  0.16435226798057556,\n",
       "  0.004627529066056013,\n",
       "  -0.05706676095724106,\n",
       "  0.040824681520462036,\n",
       "  0.019417330622673035,\n",
       "  0.0013937458861619234,\n",
       "  -0.0062041813507676125,\n",
       "  0.015232435427606106,\n",
       "  0.002503987168893218,\n",
       "  0.014786144718527794,\n",
       "  0.02266574651002884,\n",
       "  0.03634713590145111,\n",
       "  0.01811503805220127,\n",
       "  -0.0243045873939991,\n",
       "  -0.05007242411375046,\n",
       "  -0.0007919842028059065,\n",
       "  0.02954302355647087,\n",
       "  -0.024436280131340027,\n",
       "  0.019080784171819687,\n",
       "  -0.0006840695277787745,\n",
       "  -0.03772259131073952,\n",
       "  -0.04085394740104675,\n",
       "  -0.0385420136153698,\n",
       "  -0.04202454909682274,\n",
       "  0.00169371219817549,\n",
       "  0.006131018977612257,\n",
       "  -0.019505126401782036,\n",
       "  -0.058734867721796036,\n",
       "  0.018100406974554062,\n",
       "  0.015510452911257744,\n",
       "  -0.04345853254199028,\n",
       "  -0.0037404336035251617,\n",
       "  0.017544370144605637,\n",
       "  -0.042668379843235016,\n",
       "  0.016871275380253792,\n",
       "  0.001623293268494308,\n",
       "  -0.023938775062561035,\n",
       "  -0.004616554826498032,\n",
       "  0.028562646359205246,\n",
       "  -0.01852474920451641,\n",
       "  -0.02462650276720524,\n",
       "  0.011493830941617489,\n",
       "  -0.03368402272462845,\n",
       "  -0.005179906263947487,\n",
       "  0.002948449458926916,\n",
       "  0.025094741955399513,\n",
       "  -0.0030197827145457268,\n",
       "  0.05911531299352646,\n",
       "  -0.02565077692270279,\n",
       "  -0.0001928518177010119,\n",
       "  -0.029411330819129944,\n",
       "  0.033215783536434174,\n",
       "  0.033420637249946594,\n",
       "  0.05188685655593872,\n",
       "  -0.017441943287849426,\n",
       "  0.01483004167675972,\n",
       "  0.004349511582404375,\n",
       "  -0.08030317723751068,\n",
       "  -0.029308903962373734,\n",
       "  0.011793797835707664,\n",
       "  -0.005362812429666519,\n",
       "  0.0006735523929819465,\n",
       "  0.03617154806852341,\n",
       "  0.05393540486693382,\n",
       "  -0.09885718673467636,\n",
       "  0.05340863764286041,\n",
       "  -0.03623007610440254,\n",
       "  0.025372760370373726,\n",
       "  -0.032484155148267746,\n",
       "  0.041146598756313324,\n",
       "  -0.020046528428792953,\n",
       "  -0.05282333493232727,\n",
       "  -0.005472556222230196,\n",
       "  -0.028708970174193382,\n",
       "  0.014120365492999554,\n",
       "  -0.03151841089129448,\n",
       "  -0.03851274773478508,\n",
       "  -0.0034038859885185957,\n",
       "  0.004612896591424942,\n",
       "  -0.024553339928388596,\n",
       "  -0.030025895684957504,\n",
       "  -0.033303577452898026,\n",
       "  -0.01669568568468094,\n",
       "  -0.027845652773976326,\n",
       "  -0.029016252607107162,\n",
       "  -0.0358203686773777,\n",
       "  -0.004521443508565426,\n",
       "  0.038249362260103226,\n",
       "  -0.04615091532468796,\n",
       "  -0.0006355993682518601,\n",
       "  -0.01596406102180481,\n",
       "  -0.024055834859609604,\n",
       "  -0.022592583671212196,\n",
       "  -0.0009053861140273511,\n",
       "  0.029996631667017937,\n",
       "  -0.004009305965155363,\n",
       "  -0.022504789754748344,\n",
       "  0.0217438992112875,\n",
       "  -0.037927448749542236,\n",
       "  -0.06526096165180206,\n",
       "  -0.0006589199183508754,\n",
       "  0.011435301043093204,\n",
       "  -0.022197507321834564,\n",
       "  0.05001389607787132,\n",
       "  -0.014244741760194302,\n",
       "  -0.015627512708306313,\n",
       "  0.060402970761060715,\n",
       "  0.016607891768217087,\n",
       "  -0.005216487217694521,\n",
       "  -0.006145651452243328,\n",
       "  -0.07251868396997452,\n",
       "  -0.026411667466163635,\n",
       "  -0.005231119692325592,\n",
       "  -0.006782165262848139,\n",
       "  0.02389487810432911,\n",
       "  0.0022076789755374193,\n",
       "  0.06684127449989319,\n",
       "  -0.01513000763952732,\n",
       "  0.021143967285752296,\n",
       "  -0.003460587002336979,\n",
       "  -0.041439250111579895,\n",
       "  0.027333514764904976,\n",
       "  0.007982030510902405,\n",
       "  0.014998315833508968,\n",
       "  -0.041146598756313324,\n",
       "  -0.06555361300706863,\n",
       "  -0.009196528233587742,\n",
       "  0.037020232528448105,\n",
       "  0.06052003055810928,\n",
       "  0.013505800627171993,\n",
       "  -0.03593742847442627,\n",
       "  -0.03277680650353432,\n",
       "  0.014127681963145733,\n",
       "  -0.048960354179143906,\n",
       "  -0.003217321587726474,\n",
       "  0.015247068367898464,\n",
       "  0.05182832479476929,\n",
       "  0.0003971352707594633,\n",
       "  0.038249362260103226,\n",
       "  0.01308145746588707,\n",
       "  -0.01574457250535488,\n",
       "  -0.006588284391909838,\n",
       "  -0.033830348402261734,\n",
       "  0.09282860159873962,\n",
       "  0.017251720651984215,\n",
       "  -0.03356696292757988,\n",
       "  -0.02604585513472557,\n",
       "  0.02202191762626171,\n",
       "  -0.00045635117567144334,\n",
       "  0.004064177628606558,\n",
       "  -0.031255025416612625,\n",
       "  -0.0126644317060709,\n",
       "  0.02769932895898819,\n",
       "  -0.01751510612666607,\n",
       "  0.0399174690246582,\n",
       "  0.042990293353796005,\n",
       "  -0.038659073412418365,\n",
       "  0.04214160889387131,\n",
       "  -0.015817735344171524,\n",
       "  -0.02121713012456894,\n",
       "  0.03447417542338371,\n",
       "  -0.035995956510305405,\n",
       "  -0.02121713012456894,\n",
       "  0.020105058327317238,\n",
       "  0.048638440668582916,\n",
       "  -0.0017760200425982475,\n",
       "  -0.011940122582018375,\n",
       "  -0.032893866300582886,\n",
       "  0.03213297575712204,\n",
       "  0.08797060698270798,\n",
       "  0.011947439052164555,\n",
       "  -0.006577310152351856,\n",
       "  -0.005911531392484903,\n",
       "  0.056803375482559204,\n",
       "  -0.009964734315872192,\n",
       "  0.013915509916841984,\n",
       "  -0.02832852490246296,\n",
       "  -0.006372455041855574,\n",
       "  -0.005904214922338724,\n",
       "  -0.0010123862884938717,\n",
       "  -0.018188200891017914,\n",
       "  -0.01354969758540392,\n",
       "  0.023192517459392548,\n",
       "  0.008691706694662571,\n",
       "  0.019534392282366753,\n",
       "  -0.020046528428792953,\n",
       "  -0.03333284333348274,\n",
       "  -0.015115375630557537,\n",
       "  0.003138671861961484,\n",
       "  0.006350506097078323,\n",
       "  0.05118449404835701,\n",
       "  0.032806072384119034,\n",
       "  -0.013322893530130386,\n",
       "  0.009189211763441563,\n",
       "  -0.021656105294823647,\n",
       "  0.019607553258538246,\n",
       "  0.025080110877752304,\n",
       "  0.0013717971742153168,\n",
       "  0.02761153317987919,\n",
       "  -0.011742583476006985,\n",
       "  -0.012357148341834545,\n",
       "  -0.04518516734242439,\n",
       "  0.0375177375972271,\n",
       "  -0.0002947077446151525,\n",
       "  -0.020031897351145744,\n",
       "  -0.029601553454995155,\n",
       "  0.02472892962396145,\n",
       "  -0.014917836524546146,\n",
       "  0.009555024094879627,\n",
       "  -0.003797134617343545,\n",
       "  -0.02551908604800701,\n",
       "  -0.023236414417624474,\n",
       "  -0.013805766589939594,\n",
       "  -0.021963387727737427,\n",
       "  0.019475862383842468,\n",
       "  -0.02481672540307045,\n",
       "  -0.012832704931497574,\n",
       "  0.02730425074696541,\n",
       "  -0.048140935599803925,\n",
       "  0.012481524609029293,\n",
       "  0.001392831327393651,\n",
       "  -0.017763858661055565,\n",
       "  0.015320230275392532,\n",
       "  0.030845316126942635,\n",
       "  0.0014934298815205693,\n",
       "  0.05021874979138374,\n",
       "  0.0278895515948534,\n",
       "  -0.06098827347159386,\n",
       "  0.02986493892967701,\n",
       "  0.008069825358688831,\n",
       "  -0.01606648787856102,\n",
       "  -0.038161568343639374,\n",
       "  0.03219150751829147,\n",
       "  0.011113385669887066,\n",
       "  0.008523432537913322,\n",
       "  0.02461186982691288,\n",
       "  0.023090090602636337,\n",
       "  0.03143061697483063,\n",
       "  0.021187864243984222,\n",
       "  0.026323873549699783,\n",
       "  -0.00267774797976017,\n",
       "  -0.0038373738061636686,\n",
       "  0.02687990851700306,\n",
       "  0.013974040746688843,\n",
       "  0.01700296811759472,\n",
       "  0.023748552426695824,\n",
       "  -0.047438573092222214,\n",
       "  -0.0011449933517724276,\n",
       "  -0.006906541530042887,\n",
       "  0.024450913071632385,\n",
       "  -0.030347811058163643,\n",
       "  -0.013813083060085773,\n",
       "  0.03757626935839653,\n",
       "  0.005322572775185108,\n",
       "  -0.004627529066056013,\n",
       "  0.026397034525871277,\n",
       "  0.024450913071632385,\n",
       "  -0.04811166971921921,\n",
       "  0.0045397342182695866,\n",
       "  0.04972124472260475,\n",
       "  -0.05229656398296356,\n",
       "  0.0007663773722015321,\n",
       "  -0.0192710068076849,\n",
       "  0.0028972355648875237,\n",
       "  0.02449481002986431,\n",
       "  0.01975387893617153,\n",
       "  -0.009101416915655136,\n",
       "  0.03921510651707649,\n",
       "  -0.03772259131073952,\n",
       "  0.01250347401946783,\n",
       "  0.009181895293295383,\n",
       "  -0.06338800489902496,\n",
       "  -0.004064177628606558,\n",
       "  0.011274343356490135,\n",
       "  -0.0020668411161750555,\n",
       "  -0.01997336745262146,\n",
       "  -0.03634713590145111,\n",
       "  -0.037634797394275665,\n",
       "  -0.06660715490579605,\n",
       "  0.00889656227082014,\n",
       "  0.005549376830458641,\n",
       "  0.01719319075345993,\n",
       "  0.005779838655143976,\n",
       "  -0.04471692815423012,\n",
       "  0.04290249943733215,\n",
       "  -0.020836684852838516,\n",
       "  0.007689380086958408,\n",
       "  -0.027143292129039764,\n",
       "  -0.036083754152059555,\n",
       "  -0.028782133013010025,\n",
       "  0.011793797835707664,\n",
       "  0.001986362272873521,\n",
       "  -0.02843095362186432,\n",
       "  0.03415226191282272,\n",
       "  -0.016827378422021866,\n",
       "  -0.0007512875599786639,\n",
       "  -0.0073638069443404675,\n",
       "  -0.005190880503505468,\n",
       "  0.021656105294823647,\n",
       "  -0.005238436162471771,\n",
       "  -0.05463776737451553,\n",
       "  -0.018305260688066483,\n",
       "  -0.020426973700523376,\n",
       "  0.02510937489569187,\n",
       "  0.009233109652996063,\n",
       "  -0.029162578284740448,\n",
       "  0.034327853471040726,\n",
       "  -0.026733582839369774,\n",
       "  -0.027157925069332123,\n",
       "  -0.005809103604406118,\n",
       "  -0.04898962005972862,\n",
       "  0.039712611585855484,\n",
       "  0.014039886184036732,\n",
       "  -0.05876413360238075,\n",
       "  0.01451544277369976,\n",
       "  0.028138304129242897,\n",
       "  0.050101689994335175,\n",
       "  -0.00889656227082014,\n",
       "  -0.057213086634874344,\n",
       "  0.0010882924543693662,\n",
       "  0.006661446765065193,\n",
       "  -0.016198180615901947,\n",
       "  0.026938438415527344,\n",
       "  0.024655766785144806,\n",
       "  0.026338504627346992,\n",
       "  0.02285596914589405,\n",
       "  -0.0017202335875481367,\n",
       "  0.029821041971445084,\n",
       "  0.004777512047439814,\n",
       "  -0.03754700347781181,\n",
       "  -0.03936143219470978,\n",
       "  0.008172253146767616,\n",
       "  0.0010352495592087507,\n",
       "  0.012554687447845936,\n",
       "  0.01988557167351246,\n",
       "  -0.023119354620575905,\n",
       "  -0.01823209784924984,\n",
       "  0.08112259954214096,\n",
       "  -0.008742920123040676,\n",
       "  0.009562340565025806,\n",
       "  0.04749710485339165,\n",
       "  0.0484335832297802,\n",
       "  0.0004042228974867612,\n",
       "  0.04676548019051552,\n",
       "  0.01234983280301094,\n",
       "  -0.0376933291554451,\n",
       "  -0.009555024094879627,\n",
       "  0.014281323179602623,\n",
       "  -0.02244625985622406,\n",
       "  -0.025416657328605652,\n",
       "  -0.01668105274438858,\n",
       "  0.005055529996752739,\n",
       "  0.014647135511040688,\n",
       "  -0.038366422057151794,\n",
       "  0.002354003954678774,\n",
       "  0.018992988392710686,\n",
       "  -0.035381391644477844,\n",
       "  -0.020061161369085312,\n",
       "  -0.02389487810432911,\n",
       "  0.027333514764904976,\n",
       "  0.031137965619564056,\n",
       "  0.025153271853923798,\n",
       "  -0.03529359772801399,\n",
       "  -0.009262374602258205,\n",
       "  -0.00982572603970766,\n",
       "  0.01606648787856102,\n",
       "  -0.009569657035171986,\n",
       "  0.039683349430561066,\n",
       "  -0.0026027564890682697,\n",
       "  -0.007616217713803053,\n",
       "  0.014573972672224045,\n",
       "  -0.031869590282440186,\n",
       "  0.02954302355647087,\n",
       "  0.021597573533654213,\n",
       "  -0.030113691464066505,\n",
       "  0.043400004506111145,\n",
       "  -0.03637640178203583,\n",
       "  -0.034357115626335144,\n",
       "  -0.004016621969640255,\n",
       "  -0.03046487085521221,\n",
       "  0.02636777050793171,\n",
       "  0.031869590282440186,\n",
       "  -0.04018085449934006,\n",
       "  -0.003954433836042881,\n",
       "  0.07954228669404984,\n",
       "  0.027333514764904976,\n",
       "  -0.010967060923576355,\n",
       "  -0.014405699446797371,\n",
       "  0.006533412728458643,\n",
       "  -0.02761153317987919,\n",
       "  -0.04129292443394661,\n",
       "  -0.013659441843628883,\n",
       "  -0.013922826386988163,\n",
       "  -0.0013105235993862152,\n",
       "  0.02316325157880783,\n",
       "  -0.03655199334025383,\n",
       "  -0.01090853102505207,\n",
       "  0.0007316251285374165,\n",
       "  0.00741136260330677,\n",
       "  0.010593932121992111,\n",
       "  -0.026821378618478775,\n",
       "  0.031869590282440186,\n",
       "  0.02812367118895054,\n",
       "  -0.028767500072717667,\n",
       "  0.05759353190660477,\n",
       "  0.022592583671212196,\n",
       "  0.02657262608408928,\n",
       "  -0.010835368186235428,\n",
       "  0.042170874774456024,\n",
       "  -0.04187822341918945,\n",
       "  0.0448632538318634,\n",
       "  0.006277343723922968,\n",
       "  0.021904857829213142,\n",
       "  -0.03543992340564728,\n",
       "  0.01925637386739254,\n",
       "  -0.04738004505634308,\n",
       "  -0.04922373965382576,\n",
       "  0.0026228760834783316,\n",
       "  -0.0013050363631919026,\n",
       "  0.06087121367454529,\n",
       "  -0.0038702969904989004,\n",
       "  -0.020734256133437157,\n",
       "  -0.013813083060085773,\n",
       "  0.02232920005917549,\n",
       "  0.009262374602258205,\n",
       "  -0.012064498849213123,\n",
       "  -0.015364128164947033,\n",
       "  -0.006533412728458643,\n",
       "  -0.027026232331991196,\n",
       "  -0.006654130760580301,\n",
       "  0.008442954160273075,\n",
       "  -0.03611301630735397,\n",
       "  0.0277139600366354,\n",
       "  0.03482535853981972,\n",
       "  0.021334189921617508,\n",
       "  -0.040327176451683044,\n",
       "  -0.0004145113634876907,\n",
       "  -0.016520095989108086,\n",
       "  0.03233783319592476,\n",
       "  -0.006182232405990362,\n",
       "  -0.018773501738905907,\n",
       "  -0.03339137136936188,\n",
       "  -0.007382097654044628,\n",
       "  -0.0011971216881647706,\n",
       "  0.011801113374531269,\n",
       "  -0.009452597238123417,\n",
       "  -0.0232803113758564,\n",
       "  -0.007535738870501518,\n",
       "  -0.03860054165124893,\n",
       "  -0.02244625985622406,\n",
       "  -0.012086447328329086,\n",
       "  0.09247741848230362,\n",
       "  0.03541065752506256,\n",
       "  -0.022519422695040703,\n",
       "  -0.04825799539685249,\n",
       "  0.012905867770314217,\n",
       "  -0.02038307674229145,\n",
       "  -0.018041877076029778,\n",
       "  0.013015611097216606,\n",
       "  -0.008640493266284466,\n",
       "  -0.03643493354320526,\n",
       "  -0.007762542925775051,\n",
       "  -0.0033471849747002125,\n",
       "  0.015173905529081821,\n",
       "  -0.008772185072302818,\n",
       "  -3.363761061336845e-05,\n",
       "  -0.05683264136314392,\n",
       "  -0.0024399699177592993,\n",
       "  0.039390698075294495,\n",
       "  0.010696359910070896,\n",
       "  -0.03324504569172859,\n",
       "  0.032601214945316315,\n",
       "  -0.0029594236984848976,\n",
       "  -0.013732603751122952,\n",
       "  -0.079132579267025,\n",
       "  0.004729956388473511,\n",
       "  0.017251720651984215,\n",
       "  0.004382434766739607,\n",
       "  -0.023338843137025833,\n",
       "  -0.010659778490662575,\n",
       "  -0.008830715902149677,\n",
       "  0.015466555953025818,\n",
       "  -0.005373786669224501,\n",
       "  0.01656399294734001,\n",
       "  0.0022698671091347933,\n",
       "  -0.010176905430853367,\n",
       "  -0.04158557206392288,\n",
       "  0.012174242176115513,\n",
       "  0.013688706792891026,\n",
       "  0.008984357118606567,\n",
       "  -0.02123176120221615,\n",
       "  -0.005117718130350113,\n",
       "  0.004214161075651646,\n",
       "  0.02379244938492775,\n",
       "  -0.036815378814935684,\n",
       "  0.016227446496486664,\n",
       "  0.011552360840141773,\n",
       "  -0.027260353788733482,\n",
       "  -0.019095417112112045,\n",
       "  -0.019505126401782036,\n",
       "  0.004075151868164539,\n",
       "  0.007246747147291899,\n",
       "  -0.04199528321623802,\n",
       "  -0.0014705664943903685,\n",
       "  -0.02531423047184944,\n",
       "  0.012408362701535225,\n",
       "  0.034240055829286575,\n",
       "  0.002692380454391241,\n",
       "  0.01463250257074833,\n",
       "  0.016739582642912865,\n",
       "  0.008208834566175938,\n",
       "  0.05086258053779602,\n",
       "  0.050833314657211304,\n",
       "  -0.03131355717778206,\n",
       "  -0.02926500514149666,\n",
       "  -0.044131629168987274,\n",
       "  -0.05179905891418457,\n",
       "  -0.0027765175327658653,\n",
       "  -0.0022844995837658644,\n",
       "  -0.0252410676330328,\n",
       "  -0.08598058670759201,\n",
       "  0.016754215583205223,\n",
       "  -0.011976704001426697,\n",
       "  -0.00870633963495493,\n",
       "  0.01872960291802883,\n",
       "  0.012744910083711147,\n",
       "  0.02822609804570675,\n",
       "  -0.017427310347557068,\n",
       "  -0.01906615123152733,\n",
       "  -0.006138334982097149,\n",
       "  -0.03110870160162449,\n",
       "  -0.01935880072414875,\n",
       "  0.0091672632843256,\n",
       "  0.0066212075762450695,\n",
       "  0.01421547681093216,\n",
       "  -0.025723939761519432,\n",
       "  -0.007689380086958408,\n",
       "  0.03558624908328056,\n",
       "  0.02914794534444809,\n",
       "  0.038454215973615646,\n",
       "  0.005417684093117714,\n",
       "  0.01864180900156498,\n",
       "  -0.006943122483789921,\n",
       "  0.023631492629647255,\n",
       "  -0.01903688721358776,\n",
       "  -0.004064177628606558,\n",
       "  -0.01101827435195446,\n",
       "  0.022943764925003052,\n",
       "  -0.0034825357142835855,\n",
       "  0.011662105098366737,\n",
       "  -0.01801261119544506,\n",
       "  0.03470829874277115,\n",
       "  0.0069394647143781185,\n",
       "  0.001876618480309844,\n",
       "  0.00550913717597723,\n",
       "  0.027026232331991196,\n",
       "  -0.005984693765640259,\n",
       "  -0.0426391139626503,\n",
       "  0.026119017973542213,\n",
       "  0.014317904599010944,\n",
       "  -0.04284396767616272,\n",
       "  -0.023236414417624474,\n",
       "  0.03745920583605766,\n",
       "  -0.01831989362835884,\n",
       "  0.0113840876147151,\n",
       "  -0.003910536412149668,\n",
       "  -0.004532417748123407,\n",
       "  -0.004905546549707651,\n",
       "  -0.007093105930835009,\n",
       "  0.008845347911119461,\n",
       "  0.04129292443394661,\n",
       "  -0.01323509868234396,\n",
       "  0.03965408354997635,\n",
       "  -0.01497636642307043,\n",
       "  -0.03561551123857498,\n",
       "  -0.05653999000787735,\n",
       "  0.025211801752448082,\n",
       "  -0.013213150203227997,\n",
       "  0.015422658063471317,\n",
       "  0.03646419569849968,\n",
       "  -0.04585826396942139,\n",
       "  -0.0021802429109811783,\n",
       "  0.034327853471040726,\n",
       "  -0.005454265512526035,\n",
       "  -0.03523506596684456,\n",
       "  0.007385755889117718,\n",
       "  -0.04231720045208931,\n",
       "  0.017690695822238922,\n",
       "  0.003815425094217062,\n",
       "  0.019900204613804817,\n",
       "  -0.02995273284614086,\n",
       "  0.03049413673579693,\n",
       "  -0.004506811033934355,\n",
       "  0.020953744649887085,\n",
       "  0.0039434595964848995,\n",
       "  0.014398382976651192,\n",
       "  0.006705344654619694,\n",
       "  0.032074447721242905,\n",
       "  0.0033727919217199087,\n",
       "  0.0227681752294302,\n",
       "  -0.014654451981186867,\n",
       "  0.019080784171819687,\n",
       "  0.015437291003763676,\n",
       "  -0.012649798765778542,\n",
       "  -0.0066833957098424435,\n",
       "  -0.021129334345459938,\n",
       "  0.007491841446608305,\n",
       "  -0.020880581811070442,\n",
       "  0.00299783400259912,\n",
       "  0.01875886879861355,\n",
       "  -0.020353810861706734,\n",
       "  0.012898551300168037,\n",
       "  -0.03833715617656708,\n",
       "  0.009650135412812233,\n",
       "  0.005629855673760176,\n",
       "  0.01587626524269581,\n",
       "  -0.024831358343362808,\n",
       "  0.022475523874163628,\n",
       "  -0.021758532151579857,\n",
       "  0.04767269641160965,\n",
       "  -0.018393056467175484,\n",
       "  0.0388931930065155,\n",
       "  -0.00364166428335011,\n",
       "  0.0023448585998266935,\n",
       "  0.00692849000915885,\n",
       "  -0.030435606837272644,\n",
       "  -0.016037223860621452,\n",
       "  -0.040619827806949615,\n",
       "  0.025153271853923798,\n",
       "  -0.04214160889387131,\n",
       "  0.014266690239310265,\n",
       "  0.0036892197094857693,\n",
       "  0.03175253048539162,\n",
       "  0.05375981703400612,\n",
       "  0.00012048951612086967,\n",
       "  -0.043663389980793,\n",
       "  -0.004495836328715086,\n",
       "  0.007144319359213114,\n",
       "  -0.019549023360013962,\n",
       "  0.00617491640150547,\n",
       "  -0.006997994612902403,\n",
       "  -0.0026722608599811792,\n",
       "  -0.04819946363568306,\n",
       "  0.03286460041999817,\n",
       "  -0.0005985608440823853,\n",
       "  0.02050013653934002,\n",
       "  -0.02707013115286827,\n",
       "  -0.010623197071254253,\n",
       "  -0.030816052109003067,\n",
       "  -0.001905883545987308,\n",
       "  -0.01608112081885338,\n",
       "  0.03163547068834305,\n",
       "  -0.002723474521189928,\n",
       "  0.015071477741003036,\n",
       "  0.025884898379445076,\n",
       "  0.01148651447147131,\n",
       "  0.041146598756313324,\n",
       "  -0.029616186395287514,\n",
       "  0.009847674518823624,\n",
       "  -0.02194875478744507,\n",
       "  -0.015056845732033253,\n",
       "  0.012598585337400436,\n",
       "  0.017880918458104134,\n",
       "  -0.04477545991539955,\n",
       "  -0.05299892649054527,\n",
       "  0.019022254273295403,\n",
       "  0.0126205338165164,\n",
       "  -0.009394067339599133,\n",
       "  -0.013622860424220562,\n",
       "  -0.043721918016672134,\n",
       "  -0.02882602997124195,\n",
       "  -0.01169137004762888,\n",
       "  0.0018784475978463888,\n",
       "  -0.04272690787911415,\n",
       "  0.002858825260773301,\n",
       "  0.006573651917278767,\n",
       "  -0.007850337773561478,\n",
       "  0.00221316609531641,\n",
       "  -0.020544033497571945,\n",
       "  0.02256331965327263,\n",
       "  -0.003105748677626252,\n",
       "  0.032396361231803894,\n",
       "  0.007213823962956667,\n",
       "  -0.0064200107008218765,\n",
       "  -0.018041877076029778,\n",
       "  -0.008801450952887535,\n",
       "  -0.06262711435556412,\n",
       "  -0.017529739066958427,\n",
       "  0.04146851226687431,\n",
       "  0.022490156814455986,\n",
       "  -0.009416015818715096,\n",
       "  0.006782165262848139,\n",
       "  -0.004184895660728216,\n",
       "  -0.014420331455767155,\n",
       "  0.031255025416612625,\n",
       "  -0.03596669062972069,\n",
       "  0.005593274254351854,\n",
       "  0.031342823058366776,\n",
       "  0.01265711523592472,\n",
       "  -0.005183564033359289,\n",
       "  0.004038570914417505,\n",
       "  -0.009906204417347908,\n",
       "  -0.04088321328163147,\n",
       "  -0.02449481002986431,\n",
       "  -0.0012721131788566709,\n",
       "  -0.011852327734231949,\n",
       "  0.01313267182558775,\n",
       "  0.03754700347781181,\n",
       "  0.025167904794216156,\n",
       "  -0.040824681520462036,\n",
       "  0.015276333317160606,\n",
       "  0.01975387893617153,\n",
       "  0.00834052637219429,\n",
       "  -0.023924142122268677,\n",
       "  -0.007886919192969799,\n",
       "  -0.01761753298342228,\n",
       "  0.004338537342846394,\n",
       "  0.04337073862552643,\n",
       "  -0.046589888632297516,\n",
       "  0.015817735344171524,\n",
       "  0.010491504333913326,\n",
       "  0.033215783536434174,\n",
       "  -0.0116840535774827,\n",
       "  0.004938469734042883,\n",
       "  0.018085774034261703,\n",
       "  -0.0008395398617722094,\n",
       "  -0.008486852049827576,\n",
       "  0.006789481267333031,\n",
       "  0.042668379843235016,\n",
       "  0.007616217713803053,\n",
       "  0.017471209168434143,\n",
       "  0.0010946941329166293,\n",
       "  -0.04500957950949669,\n",
       "  0.027260353788733482,\n",
       "  -0.01574457250535488,\n",
       "  0.034854620695114136,\n",
       "  -0.03087458200752735,\n",
       "  -0.010513453744351864,\n",
       "  0.012027917429804802,\n",
       "  -0.010381761007010937,\n",
       "  -0.02461186982691288,\n",
       "  0.01779312454164028,\n",
       "  0.017749225720763206,\n",
       "  -0.007733277976512909,\n",
       "  0.04018085449934006,\n",
       "  0.007806440349668264,\n",
       "  0.02892845869064331,\n",
       "  -0.031167231500148773,\n",
       "  -0.005333547480404377,\n",
       "  0.032893866300582886,\n",
       "  4.152544352109544e-05,\n",
       "  0.018685705959796906,\n",
       "  -0.03593742847442627,\n",
       "  -0.013937459327280521,\n",
       "  0.012086447328329086,\n",
       "  -0.0021326872520148754,\n",
       "  0.0007800952880643308,\n",
       "  0.002441799035295844,\n",
       "  0.023499799892306328,\n",
       "  -0.03412299603223801,\n",
       "  0.028035875409841537,\n",
       "  0.018407689407467842,\n",
       "  0.008881929330527782,\n",
       "  0.003092945320531726,\n",
       "  -0.030523400753736496,\n",
       "  0.01647619903087616,\n",
       "  0.003535578493028879,\n",
       "  0.015086110681295395,\n",
       "  0.0036325189284980297,\n",
       "  -0.017968714237213135,\n",
       "  -0.02326568029820919,\n",
       "  0.008786818012595177,\n",
       "  -0.0074186790734529495,\n",
       "  0.02872360311448574,\n",
       "  -0.028665073215961456,\n",
       "  -0.012064498849213123,\n",
       "  -0.059671346098184586,\n",
       "  0.0017805927200242877,\n",
       "  -0.03415226191282272,\n",
       "  -0.012342516332864761,\n",
       "  0.0002752739528659731,\n",
       "  0.01760290190577507,\n",
       "  0.0192710068076849,\n",
       "  -0.007769858930259943,\n",
       "  0.0008509714971296489,\n",
       "  -0.032689012587070465,\n",
       "  -0.03131355717778206,\n",
       "  -0.008889245800673962,\n",
       "  0.012283986434340477,\n",
       "  0.036815378814935684,\n",
       "  -0.02778712287545204,\n",
       "  0.030552666634321213,\n",
       "  0.017368780449032784,\n",
       "  0.029016252607107162,\n",
       "  -0.02205118164420128,\n",
       "  -0.01792481541633606,\n",
       "  0.0208074189722538,\n",
       "  -0.03339137136936188,\n",
       "  0.005640829913318157,\n",
       "  0.029879571869969368,\n",
       "  -0.011830378323793411,\n",
       "  -0.009679400362074375,\n",
       "  0.027216454967856407,\n",
       "  -0.029601553454995155,\n",
       "  -0.018085774034261703,\n",
       "  0.03842495381832123,\n",
       "  -0.0038995619397610426,\n",
       "  -0.01487393956631422,\n",
       "  0.007294302806258202,\n",
       "  0.010045213624835014,\n",
       "  -0.014039886184036732,\n",
       "  0.028708970174193382,\n",
       "  -0.016432300209999084,\n",
       "  -0.014500810764729977,\n",
       "  -0.02585563249886036,\n",
       "  0.008172253146767616,\n",
       "  0.03327431157231331,\n",
       "  -0.03933216631412506,\n",
       "  -0.00947454571723938,\n",
       "  -0.040736887603998184,\n",
       "  -0.02172926627099514,\n",
       "  0.029382066801190376,\n",
       "  0.03901025280356407,\n",
       "  -0.01513000763952732,\n",
       "  -0.017676062881946564,\n",
       "  -0.030318547040224075,\n",
       "  -0.046180181205272675,\n",
       "  0.037225086241960526,\n",
       "  0.021246394142508507,\n",
       "  -0.0009657451882958412,\n",
       "  -0.003958092071115971,\n",
       "  0.0091672632843256,\n",
       "  -0.020558666437864304,\n",
       "  0.014083784073591232,\n",
       "  0.032074447721242905,\n",
       "  -0.01483004167675972,\n",
       "  0.016505463048815727,\n",
       "  0.003987357020378113,\n",
       "  -0.011925489641726017,\n",
       "  0.005454265512526035,\n",
       "  -0.021787796169519424,\n",
       "  0.006880934350192547,\n",
       "  0.013410689309239388,\n",
       "  -0.04111733287572861,\n",
       "  0.020822051912546158,\n",
       "  0.018875928595662117,\n",
       "  0.02996736578643322,\n",
       "  -0.006580968387424946,\n",
       "  0.016095753759145737,\n",
       "  -0.037751857191324234,\n",
       "  -0.00330877467058599,\n",
       "  0.0004737272975035012,\n",
       "  0.016856644302606583,\n",
       "  0.022490156814455986,\n",
       "  -0.006123702507466078,\n",
       "  0.038161568343639374,\n",
       "  -0.0024235083255916834,\n",
       "  -0.008998989127576351,\n",
       "  -0.041029538959264755,\n",
       "  0.01518853846937418,\n",
       "  0.029323535040020943,\n",
       "  -0.010945112444460392,\n",
       "  0.014588605612516403,\n",
       "  -0.013688706792891026,\n",
       "  0.02831389382481575,\n",
       "  0.03143061697483063,\n",
       "  0.0029338167514652014,\n",
       "  -0.020748889073729515,\n",
       "  0.00823809951543808,\n",
       "  -0.01761753298342228,\n",
       "  -0.023733919486403465,\n",
       "  -5.4071671911515296e-05,\n",
       "  0.0008267364464700222,\n",
       "  -0.019490493461489677,\n",
       "  0.06186622381210327,\n",
       "  -0.023558329790830612,\n",
       "  -0.003767869435250759,\n",
       "  -0.017163926735520363,\n",
       "  0.03766406327486038,\n",
       "  0.0076820640824735165,\n",
       "  0.029001621529459953,\n",
       "  0.020046528428792953,\n",
       "  0.004034912679344416,\n",
       "  -0.009840358048677444,\n",
       "  0.009379434399306774,\n",
       "  -0.014544707722961903,\n",
       "  -0.021758532151579857,\n",
       "  0.01678348146378994,\n",
       "  0.012415679171681404,\n",
       "  0.0030399025417864323,\n",
       "  0.03333284333348274,\n",
       "  0.033625490963459015,\n",
       "  0.01668105274438858,\n",
       "  -0.00504455529153347,\n",
       "  -0.007711329031735659,\n",
       "  0.033742550760507584,\n",
       "  -0.030201485380530357,\n",
       "  0.0006109069800004363,\n",
       "  0.014998315833508968,\n",
       "  -0.012876602821052074,\n",
       "  0.006013958714902401,\n",
       "  0.001484284526668489,\n",
       "  -0.012049865908920765,\n",
       "  0.06760216504335403,\n",
       "  0.008969724178314209,\n",
       "  0.029601553454995155,\n",
       "  0.03921510651707649,\n",
       "  0.00978914462029934,\n",
       "  0.021261027082800865,\n",
       "  0.013688706792891026,\n",
       "  -0.00026795771555043757,\n",
       "  0.015042212791740894,\n",
       "  0.013608227483928204,\n",
       "  0.012679063715040684,\n",
       "  0.006997994612902403,\n",
       "  -0.01101827435195446,\n",
       "  0.034620501101017,\n",
       "  -0.016417669132351875,\n",
       "  -0.01968071609735489,\n",
       "  -0.003486193949356675,\n",
       "  -0.010952428914606571,\n",
       "  0.017763858661055565,\n",
       "  -0.016403036192059517,\n",
       "  0.04986757040023804,\n",
       "  0.0020192854572087526,\n",
       "  0.006745583843439817,\n",
       "  -0.0030636803712695837,\n",
       "  0.020851315930485725,\n",
       "  -0.006266369484364986,\n",
       "  0.034971680492162704,\n",
       "  0.0030856290832161903,\n",
       "  -0.00807714182883501,\n",
       "  0.01627134345471859,\n",
       "  -0.022416993975639343,\n",
       "  0.028035875409841537,\n",
       "  0.0031203811522573233,\n",
       "  0.00267774797976017,\n",
       "  -0.04304882511496544,\n",
       "  0.005772522650659084,\n",
       "  -0.020061161369085312,\n",
       "  0.03602522239089012,\n",
       "  0.014793460257351398,\n",
       "  -0.03950775787234306,\n",
       "  -0.005911531392484903,\n",
       "  -0.04767269641160965,\n",
       "  -0.007155294064432383,\n",
       "  -0.010630513541400433,\n",
       "  ...]]"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "encoder(['hello hallo hola salut'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "5844f0dd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1536"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dims = len(encoder(['hello hallo hola salut'])[0])\n",
    "dims"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "06f83cd0",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pinecone import Pinecone, ServerlessSpec\n",
    "\n",
    "# Check if 'PINECONE_API_KEY' is set; prompt if not\n",
    "api_key = os.getenv('PINECONE_API_KEY') or getpass('Pinecone API key: ')\n",
    "\n",
    "# Initialize the Pinecone client\n",
    "pc = Pinecone(api_key=api_key)\n",
    "\n",
    "# Define the serverless specification for Pinecone (AWS region 'us-east-1')\n",
    "spec = ServerlessSpec(\n",
    "    cloud='aws', \n",
    "    region='us-east-1'\n",
    ")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "d37687e7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'_response_info': {'raw_headers': {'connection': 'keep-alive',\n",
       "                                    'content-length': '151',\n",
       "                                    'content-type': 'application/json',\n",
       "                                    'date': 'Thu, 05 Feb 2026 10:22:14 GMT',\n",
       "                                    'grpc-status': '0',\n",
       "                                    'server': 'envoy',\n",
       "                                    'x-envoy-upstream-service-time': '40',\n",
       "                                    'x-pinecone-request-latency-ms': '47',\n",
       "                                    'x-pinecone-response-duration-ms': '49'}},\n",
       " 'dimension': 1536,\n",
       " 'index_fullness': 0.0,\n",
       " 'memoryFullness': 0.0,\n",
       " 'metric': 'cosine',\n",
       " 'namespaces': {},\n",
       " 'storageFullness': 0.0,\n",
       " 'total_vector_count': 0,\n",
       " 'vector_type': 'dense'}"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import time\n",
    "\n",
    "# Define the name of the index\n",
    "index_name = 'langgraph-research-agent'\n",
    "\n",
    "# Check if the index exists; create it if it doesn't\n",
    "if index_name not in pc.list_indexes().names():\n",
    "    pc.create_index(\n",
    "        index_name,\n",
    "        dimension=dims,  # Embedding dimension (1536)\n",
    "        metric='cosine',\n",
    "        spec=spec  # Cloud provider and region specification\n",
    "    )\n",
    "\n",
    "    # Wait until the index is fully initialized\n",
    "    while True:\n",
    "        description = pc.describe_index(index_name)\n",
    "        if description and description.status and description.status.get('ready'):\n",
    "            break\n",
    "        time.sleep(1)\n",
    "\n",
    "# Connect to the index\n",
    "index = pc.Index(index_name)\n",
    "\n",
    "# Add a short delay before checking the stats\n",
    "time.sleep(1)\n",
    "\n",
    "# View the index statistics\n",
    "index.describe_index_stats()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "5d90bb45",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>title</th>\n",
       "      <th>summary</th>\n",
       "      <th>authors</th>\n",
       "      <th>arxiv_id</th>\n",
       "      <th>url</th>\n",
       "      <th>chunk</th>\n",
       "      <th>prechunk_id</th>\n",
       "      <th>postchunk_id</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2012.12104v1#0</td>\n",
       "      <td>A Deep Reinforcement Learning Approach for Ram...</td>\n",
       "      <td>Ramp metering that uses traffic signals to reg...</td>\n",
       "      <td>[Bing Liu, Yu Tang, Yuxiong Ji, Yu Shen, Yuchu...</td>\n",
       "      <td>2012.12104v1</td>\n",
       "      <td>http://arxiv.org/abs/2012.12104v1</td>\n",
       "      <td>1 \\nA Deep Reinforcement Learning Approach for...</td>\n",
       "      <td></td>\n",
       "      <td>2012.12104v1#1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2012.12104v1#1</td>\n",
       "      <td>A Deep Reinforcement Learning Approach for Ram...</td>\n",
       "      <td>Ramp metering that uses traffic signals to reg...</td>\n",
       "      <td>[Bing Liu, Yu Tang, Yuxiong Ji, Yu Shen, Yuchu...</td>\n",
       "      <td>2012.12104v1</td>\n",
       "      <td>http://arxiv.org/abs/2012.12104v1</td>\n",
       "      <td>Abstract \\nRamp metering that uses traffic sig...</td>\n",
       "      <td>2012.12104v1#0</td>\n",
       "      <td>2012.12104v1#2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2012.12104v1#2</td>\n",
       "      <td>A Deep Reinforcement Learning Approach for Ram...</td>\n",
       "      <td>Ramp metering that uses traffic signals to reg...</td>\n",
       "      <td>[Bing Liu, Yu Tang, Yuxiong Ji, Yu Shen, Yuchu...</td>\n",
       "      <td>2012.12104v1</td>\n",
       "      <td>http://arxiv.org/abs/2012.12104v1</td>\n",
       "      <td>and provide more detailed traffic information....</td>\n",
       "      <td>2012.12104v1#1</td>\n",
       "      <td>2012.12104v1#3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2012.12104v1#3</td>\n",
       "      <td>A Deep Reinforcement Learning Approach for Ram...</td>\n",
       "      <td>Ramp metering that uses traffic signals to reg...</td>\n",
       "      <td>[Bing Liu, Yu Tang, Yuxiong Ji, Yu Shen, Yuchu...</td>\n",
       "      <td>2012.12104v1</td>\n",
       "      <td>http://arxiv.org/abs/2012.12104v1</td>\n",
       "      <td>method results in 1) lower travel times in the...</td>\n",
       "      <td>2012.12104v1#2</td>\n",
       "      <td>2012.12104v1#4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2012.12104v1#4</td>\n",
       "      <td>A Deep Reinforcement Learning Approach for Ram...</td>\n",
       "      <td>Ramp metering that uses traffic signals to reg...</td>\n",
       "      <td>[Bing Liu, Yu Tang, Yuxiong Ji, Yu Shen, Yuchu...</td>\n",
       "      <td>2012.12104v1</td>\n",
       "      <td>http://arxiv.org/abs/2012.12104v1</td>\n",
       "      <td>2 \\nIntroduction \\nRamp metering uses traffic ...</td>\n",
       "      <td>2012.12104v1#3</td>\n",
       "      <td>2012.12104v1#5</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "               id                                              title  \\\n",
       "0  2012.12104v1#0  A Deep Reinforcement Learning Approach for Ram...   \n",
       "1  2012.12104v1#1  A Deep Reinforcement Learning Approach for Ram...   \n",
       "2  2012.12104v1#2  A Deep Reinforcement Learning Approach for Ram...   \n",
       "3  2012.12104v1#3  A Deep Reinforcement Learning Approach for Ram...   \n",
       "4  2012.12104v1#4  A Deep Reinforcement Learning Approach for Ram...   \n",
       "\n",
       "                                             summary  \\\n",
       "0  Ramp metering that uses traffic signals to reg...   \n",
       "1  Ramp metering that uses traffic signals to reg...   \n",
       "2  Ramp metering that uses traffic signals to reg...   \n",
       "3  Ramp metering that uses traffic signals to reg...   \n",
       "4  Ramp metering that uses traffic signals to reg...   \n",
       "\n",
       "                                             authors      arxiv_id  \\\n",
       "0  [Bing Liu, Yu Tang, Yuxiong Ji, Yu Shen, Yuchu...  2012.12104v1   \n",
       "1  [Bing Liu, Yu Tang, Yuxiong Ji, Yu Shen, Yuchu...  2012.12104v1   \n",
       "2  [Bing Liu, Yu Tang, Yuxiong Ji, Yu Shen, Yuchu...  2012.12104v1   \n",
       "3  [Bing Liu, Yu Tang, Yuxiong Ji, Yu Shen, Yuchu...  2012.12104v1   \n",
       "4  [Bing Liu, Yu Tang, Yuxiong Ji, Yu Shen, Yuchu...  2012.12104v1   \n",
       "\n",
       "                                 url  \\\n",
       "0  http://arxiv.org/abs/2012.12104v1   \n",
       "1  http://arxiv.org/abs/2012.12104v1   \n",
       "2  http://arxiv.org/abs/2012.12104v1   \n",
       "3  http://arxiv.org/abs/2012.12104v1   \n",
       "4  http://arxiv.org/abs/2012.12104v1   \n",
       "\n",
       "                                               chunk     prechunk_id  \\\n",
       "0  1 \\nA Deep Reinforcement Learning Approach for...                   \n",
       "1  Abstract \\nRamp metering that uses traffic sig...  2012.12104v1#0   \n",
       "2  and provide more detailed traffic information....  2012.12104v1#1   \n",
       "3  method results in 1) lower travel times in the...  2012.12104v1#2   \n",
       "4  2 \\nIntroduction \\nRamp metering uses traffic ...  2012.12104v1#3   \n",
       "\n",
       "     postchunk_id  \n",
       "0  2012.12104v1#1  \n",
       "1  2012.12104v1#2  \n",
       "2  2012.12104v1#3  \n",
       "3  2012.12104v1#4  \n",
       "4  2012.12104v1#5  "
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "expanded_df.iloc[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "666bb854",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "805e6fec21fd450db85402ebfaa3ceeb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/35 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2026-02-05 05:27:15 - urllib3.connectionpool - WARNING - connectionpool.py:868 - urlopen() - Retrying (JitterRetry(total=4, connect=None, read=None, redirect=None, status=None)) after connection broken by 'ConnectionResetError(54, 'Connection reset by peer')': /vectors/upsert\n",
      "2026-02-05 05:27:17 - urllib3.connectionpool - WARNING - connectionpool.py:868 - urlopen() - Retrying (JitterRetry(total=4, connect=None, read=None, redirect=None, status=None)) after connection broken by 'ConnectionResetError(54, 'Connection reset by peer')': /vectors/upsert\n",
      "2026-02-05 05:27:23 - urllib3.connectionpool - WARNING - connectionpool.py:868 - urlopen() - Retrying (JitterRetry(total=4, connect=None, read=None, redirect=None, status=None)) after connection broken by 'ConnectionResetError(54, 'Connection reset by peer')': /vectors/upsert\n",
      "2026-02-05 05:27:25 - urllib3.connectionpool - WARNING - connectionpool.py:868 - urlopen() - Retrying (JitterRetry(total=4, connect=None, read=None, redirect=None, status=None)) after connection broken by 'ConnectionResetError(54, 'Connection reset by peer')': /vectors/upsert\n",
      "2026-02-05 05:27:29 - urllib3.connectionpool - WARNING - connectionpool.py:868 - urlopen() - Retrying (JitterRetry(total=4, connect=None, read=None, redirect=None, status=None)) after connection broken by 'ConnectionResetError(54, 'Connection reset by peer')': /vectors/upsert\n",
      "2026-02-05 05:27:31 - urllib3.connectionpool - WARNING - connectionpool.py:868 - urlopen() - Retrying (JitterRetry(total=4, connect=None, read=None, redirect=None, status=None)) after connection broken by 'ConnectionResetError(54, 'Connection reset by peer')': /vectors/upsert\n",
      "2026-02-05 05:27:47 - urllib3.connectionpool - WARNING - connectionpool.py:868 - urlopen() - Retrying (JitterRetry(total=4, connect=None, read=None, redirect=None, status=None)) after connection broken by 'ConnectionResetError(54, 'Connection reset by peer')': /vectors/upsert\n"
     ]
    }
   ],
   "source": [
    "from tqdm.auto import tqdm\n",
    "\n",
    "data = expanded_df\n",
    "batch_size = 64  # Set batch size\n",
    "\n",
    "# Loop through the data in batches, using tqdm for a progress bar\n",
    "for i in tqdm(range(0, len(data), batch_size)):\n",
    "    i_end = min(len(data), i + batch_size)  # Define batch endpoint\n",
    "    batch = data[i:i_end].to_dict(orient='records')  # Slice data into a batch\n",
    "\n",
    "    # Extract metadata for each chunk in the batch\n",
    "    metadata = [{\n",
    "        'arxiv_id': r['arxiv_id'],\n",
    "        'title': r['title'],\n",
    "        'chunk': r['chunk'],\n",
    "    } for r in batch]\n",
    "    \n",
    "    # Generate unique IDs for each chunk\n",
    "    ids = [r['id'] for r in batch]\n",
    "    \n",
    "    # Extract the chunk content\n",
    "    chunks = [r['chunk'] for r in batch]\n",
    "    \n",
    "    # Convert chunks into embeddings\n",
    "    embeds = encoder(chunks)\n",
    "    \n",
    "    # Upload embeddings, IDs, and metadata to Pinecone\n",
    "    index.upsert(vectors=list(zip(ids, embeds, metadata)))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "75cb4d8b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'_response_info': {'raw_headers': {'connection': 'keep-alive',\n",
       "                                    'content-length': '188',\n",
       "                                    'content-type': 'application/json',\n",
       "                                    'date': 'Thu, 05 Feb 2026 10:28:32 GMT',\n",
       "                                    'grpc-status': '0',\n",
       "                                    'server': 'envoy',\n",
       "                                    'x-envoy-upstream-service-time': '49',\n",
       "                                    'x-pinecone-request-latency-ms': '48',\n",
       "                                    'x-pinecone-response-duration-ms': '50'}},\n",
       " 'dimension': 1536,\n",
       " 'index_fullness': 0.0,\n",
       " 'memoryFullness': 0.0,\n",
       " 'metric': 'cosine',\n",
       " 'namespaces': {'__default__': {'vector_count': 2200}},\n",
       " 'storageFullness': 0.0,\n",
       " 'total_vector_count': 2200,\n",
       " 'vector_type': 'dense'}"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Display the index statistics.\n",
    "index.describe_index_stats()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
